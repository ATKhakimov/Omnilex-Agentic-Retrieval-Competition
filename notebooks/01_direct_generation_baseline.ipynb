{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct Generation Baseline for Omnilex Legal Retrieval\n",
    "\n",
    "This notebook implements a **direct generation approach** where we prompt a local LLM to generate Swiss legal citations based on the query.\n",
    "\n",
    "## Approach\n",
    "1. Load a local LLM (GGUF format via llama-cpp-python)\n",
    "2. For each query, prompt the LLM to directly generate relevant citations\n",
    "3. Parse and normalize the generated citations\n",
    "4. Create submission file\n",
    "\n",
    "## Requirements\n",
    "- llama-cpp-python (optional - runs in mock mode if not installed)\n",
    "- A GGUF model file (optional - uses mock responses for demonstration if not available)\n",
    "- pandas, tqdm\n",
    "\n",
    "**Note**: This notebook runs in \"mock mode\" if llama-cpp-python is not installed or no model file is available, generating placeholder citations for demonstration purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "# Choose which dataset to run on: \"val\" or \"test\"\n",
    "DATASET_MODE = \"val\"  # Change to \"test\" for final submission\n",
    "\n",
    "# Detect environment\n",
    "KAGGLE_ENV = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "\n",
    "if KAGGLE_ENV:\n",
    "    # Kaggle paths\n",
    "    DATA_PATH = Path(\"/kaggle/input/omnilex-data\")\n",
    "    MODEL_PATH = Path(\"/kaggle/input/llama-model\")\n",
    "    OUTPUT_PATH = Path(\"/kaggle/working\")\n",
    "    sys.path.insert(0, \"/kaggle/input/omnilex-utils\")\n",
    "else:\n",
    "    # Local development paths\n",
    "    REPO_ROOT = Path(\".\").resolve().parent\n",
    "    DATA_PATH = REPO_ROOT / \"data\"\n",
    "    MODEL_PATH = REPO_ROOT / \"models\"\n",
    "    OUTPUT_PATH = REPO_ROOT / \"output\"\n",
    "    sys.path.insert(0, str(REPO_ROOT / \"src\"))\n",
    "\n",
    "# Derived paths based on DATASET_MODE\n",
    "QUERY_FILE = DATA_PATH / f\"{DATASET_MODE}.csv\"\n",
    "IS_VALIDATION_MODE = DATASET_MODE == \"val\"\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Environment: {'Kaggle' if KAGGLE_ENV else 'Local'}\")\n",
    "print(f\"Dataset mode: {DATASET_MODE}\")\n",
    "print(f\"Query file: {QUERY_FILE}\")\n",
    "print(f\"Validation mode: {IS_VALIDATION_MODE}\")\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Model path: {MODEL_PATH}\")\n",
    "print(f\"Output path: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Model settings\n",
    "    \"model_file\": \"mistral-7b-instruct-v0.2.Q4_K_M.gguf\",  # Adjust to your model\n",
    "    \"n_ctx\": 4096,         # Context window\n",
    "    \"n_threads\": 4,        # CPU threads\n",
    "    \"n_gpu_layers\": -1,    # GPU layers (-1 = offload all layers to GPU)\n",
    "    \n",
    "    # Generation settings\n",
    "    \"max_tokens\": 512,\n",
    "    \"temperature\": 0.0,    # Low temperature for consistency\n",
    "    \n",
    "    # Paths\n",
    "    \"test_file\": \"test.csv\",\n",
    "    \"train_file\": \"train.csv\",  # For local evaluation\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import llama_cpp, with fallback for environments without it\n",
    "try:\n",
    "    from llama_cpp import Llama\n",
    "    LLAMA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LLAMA_AVAILABLE = False\n",
    "    print(\"llama_cpp not available. Using mock LLM for demonstration.\")\n",
    "\n",
    "llm = None\n",
    "\n",
    "if LLAMA_AVAILABLE:\n",
    "    # Find model file\n",
    "    model_file = MODEL_PATH / CONFIG[\"model_file\"]\n",
    "\n",
    "    if not model_file.exists():\n",
    "        # Try to find any GGUF file\n",
    "        gguf_files = list(MODEL_PATH.glob(\"*.gguf\")) + list(MODEL_PATH.rglob(\"*.gguf\"))\n",
    "        if gguf_files:\n",
    "            model_file = gguf_files[0]\n",
    "            print(f\"Using model: {model_file}\")\n",
    "        else:\n",
    "            print(f\"No model found in {MODEL_PATH}. Using mock LLM for demonstration.\")\n",
    "            model_file = None\n",
    "\n",
    "    if model_file and model_file.exists():\n",
    "        print(f\"Loading model: {model_file}\")\n",
    "        llm = Llama(\n",
    "            model_path=str(model_file),\n",
    "            n_ctx=CONFIG[\"n_ctx\"],\n",
    "            n_threads=CONFIG[\"n_threads\"],\n",
    "            n_gpu_layers=CONFIG[\"n_gpu_layers\"],\n",
    "            verbose=False,\n",
    "        )\n",
    "        print(\"Model loaded successfully!\")\n",
    "        \n",
    "        # Report compute device\n",
    "        if CONFIG[\"n_gpu_layers\"] == -1:\n",
    "            print(\"Running on: GPU (all layers offloaded)\")\n",
    "        elif CONFIG[\"n_gpu_layers\"] > 0:\n",
    "            print(f\"Running on: GPU ({CONFIG['n_gpu_layers']} layers offloaded)\")\n",
    "        else:\n",
    "            print(f\"Running on: CPU ({CONFIG['n_threads']} threads)\")\n",
    "\n",
    "if llm is None:\n",
    "    print(\"Running in mock mode - will generate placeholder citations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Generation Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a Swiss legal citation expert. Output ONLY a Python list of citations.\n",
    "\n",
    "CITATION FORMATS:\n",
    "- Federal laws: \"Art. X ABBREV\" where ABBREV is ZGB, OR, StGB, BV, etc.\n",
    "- Court decisions: \"BGE X Y Z\" or \"BGE X Y Z E. N\" with consideration number\n",
    "\n",
    "OUTPUT FORMAT: Python list like [\"citation1\", \"citation2\", ...]\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "Query: What are the requirements for a valid contract under Swiss law?\n",
    "[\"Art. 1 OR\", \"Art. 11 OR\", \"Art. 12 OR\", \"BGE 119 II 449 E. 2\", \"BGE 127 III 248 E. 3.1\"]\n",
    "\n",
    "Query: When can a marriage be annulled in Switzerland?\n",
    "[\"Art. 104 ZGB\", \"Art. 105 ZGB\", \"Art. 106 ZGB\", \"BGE 121 III 38 E. 2b\"]\n",
    "\n",
    "Query: What constitutes negligent homicide under Swiss criminal law?\n",
    "[\"Art. 117 StGB\", \"Art. 12 StGB\", \"BGE 116 IV 306 E. 1a\"]\n",
    "\n",
    "Query: What are the grounds for divorce in Swiss law?\n",
    "[\"Art. 111 ZGB\", \"Art. 112 ZGB\", \"Art. 114 ZGB\", \"Art. 115 ZGB\", \"BGE 130 III 585 E. 2.1\"]\n",
    "\n",
    "Query: How is inheritance distributed under Swiss law?\n",
    "[\"Art. 457 ZGB\", \"Art. 462 ZGB\", \"Art. 471 ZGB\", \"BGE 132 III 305 E. 3.2\"]\n",
    "\n",
    "Now answer:\"\"\"\n",
    "\n",
    "# Common Swiss law abbreviations for regex matching\n",
    "LAW_ABBREVS = (\n",
    "    \"ZGB|OR|StGB|BV|SchKG|ZPO|StPO|BGG|VwVG|IPRG|KG|DSG|MSchG|URG|PatG|\"\n",
    "    \"DesG|UWG|PrSG|FINMAG|BankG|VAG|KAG|GwG|BEHG|FinfraG|FIDLEG|FINIG|\"\n",
    "    \"ATSG|AHV|IV|EO|ALV|KVG|UVG|BVG|ArG|GlG|USG|RPG|WaG|JSG|TSchG|\"\n",
    "    \"LwG|PBG|EBG|SVG|LFG|SebG|SpG|BoeB|EMRK|SR|AS|BBl|ParlG|RVOG|RVOV|\"\n",
    "    \"MG|BPG|BPV|VBGÃ–|VDSG|MWSTG|DBG|StHG|VStG|StG|ZG|CO|CP|CC|CPC|CPP|\"\n",
    "    \"LEtr|LAsi|LN|LDIP|LCart|LDA|LPM|LBI|LDes|LCD|LFINMA|LB|LSA|LPCC|\"\n",
    "    \"LBA|LBVM|LIMF|LSFin|LEFin|LAVS|LAI|LAPG|LACI|LAMal|LAA|LPP|LTr|\"\n",
    "    \"LEg|LPD|LPE|LAT|LFo|LChP|LPN|LAgr|LTV|LCdF|LNA|LPTh|LTAF|LTF\"\n",
    ")\n",
    "\n",
    "\n",
    "def extract_citations(raw_output: str) -> list[str]:\n",
    "    \"\"\"Extract citations from raw LLM output using regex patterns.\"\"\"\n",
    "    import re\n",
    "\n",
    "    citations = []\n",
    "\n",
    "    # Pattern for BGE citations: BGE 141 II 345 E. 3.2\n",
    "    # Matches: BGE + volume + part (roman) + page + optional consideration\n",
    "    bge_pattern = r'BGE\\s+(\\d+)\\s+([IVX]+[a-z]?)\\s+(\\d+)(?:\\s+E\\.\\s*([\\d.a-z/]+))?'\n",
    "    for match in re.finditer(bge_pattern, raw_output):\n",
    "        vol, part, page, consid = match.groups()\n",
    "        if consid:\n",
    "            citations.append(f\"BGE {vol} {part} {page} E. {consid}\")\n",
    "        else:\n",
    "            citations.append(f\"BGE {vol} {part} {page}\")\n",
    "\n",
    "    # Pattern for Art. citations with Abs./lit./Ziff.\n",
    "    # Matches: Art. 221 Abs. 1 lit. b StPO, Art. 364 Abs. 1 OR, Art. 1 ZGB\n",
    "    art_pattern = rf'Art\\.?\\s*(\\d+[a-z]?)(?:\\s+(Abs\\.?\\s*\\d+))?(?:\\s+(lit\\.?\\s*[a-z]))?(?:\\s+(Ziff\\.?\\s*\\d+))?\\s+({LAW_ABBREVS})\\b'\n",
    "    for match in re.finditer(art_pattern, raw_output, re.IGNORECASE):\n",
    "        art_num, abs_part, lit_part, ziff_part, abbrev = match.groups()\n",
    "        parts = [f\"Art. {art_num}\"]\n",
    "        if abs_part:\n",
    "            # Normalize \"Abs1\" or \"Abs 1\" to \"Abs. 1\"\n",
    "            abs_normalized = re.sub(r'Abs\\.?\\s*', 'Abs. ', abs_part)\n",
    "            parts.append(abs_normalized.strip())\n",
    "        if lit_part:\n",
    "            lit_normalized = re.sub(r'lit\\.?\\s*', 'lit. ', lit_part)\n",
    "            parts.append(lit_normalized.strip())\n",
    "        if ziff_part:\n",
    "            ziff_normalized = re.sub(r'Ziff\\.?\\s*', 'Ziff. ', ziff_part)\n",
    "            parts.append(ziff_normalized.strip())\n",
    "        parts.append(abbrev.upper())\n",
    "        citations.append(\" \".join(parts))\n",
    "\n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for c in citations:\n",
    "        if c not in seen:\n",
    "            seen.add(c)\n",
    "            unique.append(c)\n",
    "\n",
    "    return unique\n",
    "\n",
    "\n",
    "def generate_citations(query: str) -> list[str]:\n",
    "    \"\"\"Generate citations using direct LLM prompting.\"\"\"\n",
    "    if llm is None:\n",
    "        raise RuntimeError(\"LLM not loaded - please ensure model is available\")\n",
    "\n",
    "    prompt = f\"[INST] {SYSTEM_PROMPT}\\n\\nQuery: {query} [/INST]\"\n",
    "\n",
    "    response = llm(\n",
    "        prompt,\n",
    "        max_tokens=CONFIG[\"max_tokens\"],\n",
    "        temperature=CONFIG[\"temperature\"],\n",
    "        stop=[\"[INST]\", \"</s>\", \"Query:\", \"\\n\\n\"],\n",
    "    )\n",
    "\n",
    "    raw_output: str = response[\"choices\"][0][\"text\"].strip()  # type: ignore[index]\n",
    "    print(f\"Raw output: {raw_output}\\n\")\n",
    "\n",
    "    # Parse Python list format first\n",
    "    import re\n",
    "    import ast\n",
    "    \n",
    "    citations = []\n",
    "    \n",
    "    # Try to parse as Python list\n",
    "    try:\n",
    "        list_match = re.search(r'\\[.*?\\]', raw_output, re.DOTALL)\n",
    "        if list_match:\n",
    "            parsed = ast.literal_eval(list_match.group())\n",
    "            if isinstance(parsed, list):\n",
    "                # Extract citations from parsed list items\n",
    "                for item in parsed:\n",
    "                    item_str = str(item).strip()\n",
    "                    # Extract citations from each item (may have descriptions in parens)\n",
    "                    extracted = extract_citations(item_str)\n",
    "                    citations.extend(extracted)\n",
    "    except (ValueError, SyntaxError):\n",
    "        pass\n",
    "    \n",
    "    # If no citations found from list parsing, try extracting from full output\n",
    "    if not citations:\n",
    "        citations = extract_citations(raw_output)\n",
    "\n",
    "    return citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation with a sample query\n",
    "test_query = \"What are the requirements for a valid contract under Swiss law?\"\n",
    "print(f\"Query: {test_query}\")\n",
    "raw_citations = generate_citations(test_query)\n",
    "print(\"\\nGenerated citations:\")\n",
    "for c in raw_citations:\n",
    "    print(f\"  - {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load queries from the configured query file\n",
    "if not QUERY_FILE.exists():\n",
    "    raise FileNotFoundError(f\"Query file not found: {QUERY_FILE}\")\n",
    "\n",
    "test_df = pd.read_csv(QUERY_FILE)\n",
    "\n",
    "print(f\"Loaded {len(test_df)} queries from {QUERY_FILE}\")\n",
    "print(f\"Columns: {list(test_df.columns)}\")\n",
    "\n",
    "if IS_VALIDATION_MODE and \"gold_citations\" in test_df.columns:\n",
    "    print(f\"Gold citations available for evaluation\")\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Generate predictions\n",
    "predictions = []\n",
    "\n",
    "assert test_df is not None, \"test_df must be loaded before generating predictions\"\n",
    "\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Generating predictions\"):\n",
    "    query_id = row[\"query_id\"]\n",
    "    query_text = row[\"query\"]\n",
    "\n",
    "    # Generate citations using regex extraction (no external normalizer needed)\n",
    "    raw_citations = generate_citations(query_text)\n",
    "\n",
    "    # Use \" \" (single space) when no citations found, otherwise join with \";\"\n",
    "    predicted = \";\".join(raw_citations) if raw_citations else \" \"\n",
    "\n",
    "    predictions.append({\n",
    "        \"query_id\": query_id,\n",
    "        \"predicted_citations\": predicted,\n",
    "    })\n",
    "\n",
    "print(f\"\\nGenerated predictions for {len(predictions)} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview predictions\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submission\n",
    "submission_path = OUTPUT_PATH / \"submission.csv\"\n",
    "predictions_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Submission saved to: {submission_path}\")\n",
    "print(f\"Total predictions: {len(predictions_df)}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample submission:\")\n",
    "print(predictions_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This baseline notebook demonstrates a simple direct generation approach:\n",
    "\n",
    "1. **Prompt engineering**: We use a structured prompt that asks the LLM to generate Swiss legal citations in standard format.\n",
    "\n",
    "2. **Citation normalization**: The generated citations are normalized to canonical form for consistent evaluation.\n",
    "\n",
    "3. **Limitations**:\n",
    "   - The LLM may hallucinate non-existent citations\n",
    "   - No access to actual legal documents for verification\n",
    "   - Relies entirely on the LLM's training data knowledge\n",
    "\n",
    "For better results, see the **Agentic Retrieval Baseline** notebook which uses search tools to ground the generation in actual legal documents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
