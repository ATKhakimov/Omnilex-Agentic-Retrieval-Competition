{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87ce3526",
   "metadata": {},
   "source": [
    "# Graph-Augmented Retrieval Pipeline Testing\n",
    "\n",
    "This notebook tests and demonstrates the full graph-augmented retrieval pipeline:\n",
    "\n",
    "1. **Ingestor** - Load and normalize corpus data\n",
    "2. **Summarizer** - Generate summaries for chunks\n",
    "3. **Embedder** - Build embeddings and indices\n",
    "4. **GraphBuilder** - Build graph edges and score candidates\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "```\n",
    "laws_de.csv + court_considerations.csv\n",
    "        ‚Üì (Ingestor)\n",
    "    chunks.parquet\n",
    "        ‚Üì (Summarizer)\n",
    "   summaries.parquet\n",
    "        ‚Üì (Embedder)\n",
    "embeddings.npy + faiss_index.bin + bm25_index.pkl\n",
    "        ‚Üì (GraphBuilder)\n",
    "edges_similar.parquet + edges_cocite.parquet + groups.parquet\n",
    "        ‚Üì (Inference)\n",
    "   predictions.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034b2022",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22051424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: Local\n",
      "Data path: C:\\Users\\Artem Khakimov\\Desktop\\Projects\\LEXam_kaggle\\Omnilex-Agentic-Retrieval-Competition\\data\n",
      "Processed path: C:\\Users\\Artem Khakimov\\Desktop\\Projects\\LEXam_kaggle\\Omnilex-Agentic-Retrieval-Competition\\data\\processed\n",
      "Output path: C:\\Users\\Artem Khakimov\\Desktop\\Projects\\LEXam_kaggle\\Omnilex-Agentic-Retrieval-Competition\\output\n",
      "\n",
      "API Configuration:\n",
      "  OPENAI_API_KEY: ***aumv\n",
      "  PROXYAPI_BASE_URL: https://api.proxyapi.ru/openai/v1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# =============================================================================\n",
    "# Environment Variables (API Configuration)\n",
    "# =============================================================================\n",
    "# Set these before running if you need LLM API access (for summarizer_mode=\"llm\")\n",
    "# You can also set them in your shell or .env file\n",
    "\n",
    "# API Key for LLM provider (e.g., OpenAI, ProxyAPI)\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"sk-UE4HD39TPIfPpVnyOuD33zxfuIJYaumv\")\n",
    "\n",
    "# ProxyAPI base URL (for Russian users or custom proxies)\n",
    "PROXYAPI_BASE_URL = os.environ.get(\"PROXYAPI_BASE_URL\", \"https://api.proxyapi.ru/openai/v1\")\n",
    "\n",
    "# Optionally set them here directly (not recommended for production)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "# os.environ[\"PROXYAPI_BASE_URL\"] = \"https://api.proxyapi.ru/openai/v1\"\n",
    "\n",
    "# =============================================================================\n",
    "# Setup paths\n",
    "# =============================================================================\n",
    "KAGGLE_ENV = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "\n",
    "if KAGGLE_ENV:\n",
    "    REPO_ROOT = Path(\"/kaggle/input/omnilex-repo\")\n",
    "    DATA_PATH = Path(\"/kaggle/input/omnilex-data\")\n",
    "    OUTPUT_PATH = Path(\"/kaggle/working\")\n",
    "else:\n",
    "    REPO_ROOT = Path(\".\").resolve().parent\n",
    "    DATA_PATH = REPO_ROOT / \"data\"\n",
    "    OUTPUT_PATH = REPO_ROOT / \"output\"\n",
    "\n",
    "PROCESSED_PATH = DATA_PATH / \"processed\"\n",
    "PROCESSED_PATH.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(REPO_ROOT / \"src\"))\n",
    "\n",
    "print(f\"Environment: {'Kaggle' if KAGGLE_ENV else 'Local'}\")\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Processed path: {PROCESSED_PATH}\")\n",
    "print(f\"Output path: {OUTPUT_PATH}\")\n",
    "print(f\"\\nAPI Configuration:\")\n",
    "print(f\"  OPENAI_API_KEY: {'***' + OPENAI_API_KEY[-4:] if len(OPENAI_API_KEY) > 4 else '(not set)'}\")\n",
    "print(f\"  PROXYAPI_BASE_URL: {PROXYAPI_BASE_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f70c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "LLM client created: provider=google, model=gemini-2.5-flash-lite\n",
      "Testing LLM connection...\n",
      "‚úÖ LLM Response: Hello, I am working!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import importlib\n",
    "\n",
    "# Reload modules to pick up code changes\n",
    "import omnilex.graph.summarizer\n",
    "import omnilex.graph.reranker\n",
    "importlib.reload(omnilex.graph.summarizer)\n",
    "importlib.reload(omnilex.graph.reranker)\n",
    "\n",
    "# Import graph modules\n",
    "from omnilex.graph.ingestor import Ingestor, ChunkType, Language\n",
    "from omnilex.graph.summarizer import Summarizer, SummaryType, create_llm_client\n",
    "from omnilex.graph.embedder import Embedder, EMBEDDING_MODEL\n",
    "from omnilex.graph.graph_builder import GraphBuilder, ExpansionParams, ScoringParams\n",
    "from omnilex.graph.reranker import LLMReranker, RerankerConfig, QueryPreprocessor\n",
    "\n",
    "print(f\"‚úÖ Modules loaded\")\n",
    "print(f\"   Embedding model: {EMBEDDING_MODEL}\")\n",
    "\n",
    "# =============================================================================\n",
    "# LLM Client Setup (–¥–ª—è LLM Rerank –Ω–∞ Kaggle)\n",
    "# =============================================================================\n",
    "# Choose provider: \"openai\" or \"google\"\n",
    "LLM_PROVIDER = \"google\"  # <-- Change this to switch providers\n",
    "\n",
    "# Create LLM client (used for reranking, NOT for summarization)\n",
    "if LLM_PROVIDER == \"google\":\n",
    "    llm_client = create_llm_client(\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        provider=\"google\",\n",
    "    )\n",
    "else:\n",
    "    llm_client = create_llm_client(\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        model=\"gpt-4o\",\n",
    "        provider=\"openai\",\n",
    "    )\n",
    "\n",
    "print(f\"\\nü§ñ LLM client: provider={llm_client.provider}, model={llm_client.model}\")\n",
    "\n",
    "# Test LLM connection\n",
    "print(\"   Testing connection...\", end=\" \")\n",
    "try:\n",
    "    response = llm_client(\"Say 'OK' if working.\")\n",
    "    print(f\"‚úÖ {response[:50]}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965fa07b",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d6e9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  force_rebuild: False\n",
      "  sample_size: None\n",
      "  summarizer_mode: llm\n",
      "  embedding_model: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "  embedding_batch_size: 64\n",
      "  faiss_index_type: flat\n",
      "  similar_k: 50\n",
      "  similar_min_cos: 0.25\n",
      "  cocite_top_m: 50\n",
      "  k_expand_sim: 20\n",
      "  k_expand_cocite: 30\n",
      "  k_expand_siblings: 10\n",
      "  max_candidates: 800\n",
      "  alpha: 1.0\n",
      "  beta: 0.6\n",
      "  gamma: 0.8\n",
      "  delta: 0.2\n",
      "  top_k_retrieval: 100\n",
      "  top_k_final: 20\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Pipeline Configuration (—Å–æ–≥–ª–∞—Å–Ω–æ –∏—Ç–æ–≥–æ–≤–æ–º—É –ø–ª–∞–Ω—É)\n",
    "# =============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # ---------------------------------------------------------------------\n",
    "    # –≠—Ç–∞–ø 0: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤\n",
    "    # ---------------------------------------------------------------------\n",
    "    \"force_rebuild\": False,     # True = –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –≤—Å–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã\n",
    "    \"sample_size\": None,        # int –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä 1000)\n",
    "    \n",
    "    # Summarizer: –í–°–ï–ì–î–ê heuristic –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞ (LLM —Å–ª–∏—à–∫–æ–º –¥–æ—Ä–æ–≥–æ)\n",
    "    \"summarizer_mode\": \"heuristic\",  # \"heuristic\" - –æ—Ñ–ª–∞–π–Ω, –±—ã—Å—Ç—Ä–æ\n",
    "    \n",
    "    # Embedder\n",
    "    \"embedding_model\": EMBEDDING_MODEL,\n",
    "    \"embedding_batch_size\": 64,\n",
    "    \"faiss_index_type\": \"flat\",  # \"flat\", \"ivf\", \"hnsw\"\n",
    "    \n",
    "    # Graph Builder\n",
    "    \"similar_k\": 50,             # SIMILAR_TO: k —Å–æ—Å–µ–¥–µ–π\n",
    "    \"similar_min_cos\": 0.25,     # –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∫–æ—Å–∏–Ω—É—Å\n",
    "    \"cocite_top_m\": 50,          # CO_CITED_WITH: topM —Å–æ—Å–µ–¥–µ–π –Ω–∞ —É–∑–µ–ª\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # –≠—Ç–∞–ø 1: Inference (–Ω–∞ Kaggle)\n",
    "    # ---------------------------------------------------------------------\n",
    "    \n",
    "    # Initial Retrieval\n",
    "    \"top_n_bm25\": 200,           # BM25 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤\n",
    "    \"top_n_faiss\": 200,          # FAISS –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤\n",
    "    \"top_k_retrieval\": 200,      # –ø–æ—Å–ª–µ RRF fusion\n",
    "    \n",
    "    # Graph Expansion\n",
    "    \"k_expand_sim\": 20,          # —Å–æ—Å–µ–¥–µ–π –ø–æ SIMILAR_TO\n",
    "    \"k_expand_cocite\": 30,       # —Å–æ—Å–µ–¥–µ–π –ø–æ CO_CITED_WITH\n",
    "    \"k_expand_siblings\": 10,     # siblings –∏–∑ DocGroup\n",
    "    \"max_candidates\": 800,       # cap –ø–æ—Å–ª–µ expansion\n",
    "    \n",
    "    # Fast Scoring (–ª–∏–Ω–µ–π–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è)\n",
    "    \"alpha\": 1.0,    # retrieval score weight\n",
    "    \"beta\": 0.6,     # similarity edge weight\n",
    "    \"gamma\": 0.8,    # co-citation weight\n",
    "    \"delta\": 0.2,    # docgroup bonus\n",
    "    \n",
    "    # LLM Rerank (—Ç–æ—á–µ—á–Ω–æ –Ω–∞ topK)\n",
    "    \"use_llm_rerank\": True,      # –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LLM –¥–ª—è rerank\n",
    "    \"top_k_to_rerank\": 100,      # –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è LLM rerank\n",
    "    \"rerank_batch_size\": 20,     # batch size –¥–ª—è LLM\n",
    "    \"relevance_threshold\": 0.5,  # –ø–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (0-1)\n",
    "    \n",
    "    # Final Output\n",
    "    \"top_k_final\": 20,           # —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö citations\n",
    "    \n",
    "    # Query Preprocessing\n",
    "    \"use_query_summary\": False,  # LLM summary –¥–ª—è query (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PIPELINE CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüì¶ –≠—Ç–∞–ø 0 - –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤:\")\n",
    "print(f\"  summarizer_mode: {CONFIG['summarizer_mode']}\")\n",
    "print(f\"  embedding_model: {CONFIG['embedding_model']}\")\n",
    "print(f\"  similar_k: {CONFIG['similar_k']}, cocite_top_m: {CONFIG['cocite_top_m']}\")\n",
    "\n",
    "print(\"\\nüîç –≠—Ç–∞–ø 1 - Retrieval:\")\n",
    "print(f\"  BM25 top: {CONFIG['top_n_bm25']}, FAISS top: {CONFIG['top_n_faiss']}\")\n",
    "print(f\"  Graph expansion: sim={CONFIG['k_expand_sim']}, cocite={CONFIG['k_expand_cocite']}, siblings={CONFIG['k_expand_siblings']}\")\n",
    "print(f\"  Max candidates: {CONFIG['max_candidates']}\")\n",
    "\n",
    "print(\"\\n‚öñÔ∏è Scoring weights:\")\n",
    "print(f\"  Œ±={CONFIG['alpha']} (retrieval), Œ≤={CONFIG['beta']} (similarity)\")\n",
    "print(f\"  Œ≥={CONFIG['gamma']} (co-citation), Œ¥={CONFIG['delta']} (docgroup)\")\n",
    "\n",
    "print(\"\\nü§ñ LLM Rerank:\")\n",
    "print(f\"  use_llm_rerank: {CONFIG['use_llm_rerank']}\")\n",
    "print(f\"  top_k_to_rerank: {CONFIG['top_k_to_rerank']}\")\n",
    "print(f\"  top_k_final: {CONFIG['top_k_final']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fbf8e9",
   "metadata": {},
   "source": [
    "## 3. Step 1: Ingestor - Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bcebf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing chunks from C:\\Users\\Artem Khakimov\\Desktop\\Projects\\LEXam_kaggle\\Omnilex-Agentic-Retrieval-Competition\\data\\processed\\chunks.parquet\n",
      "\n",
      "Chunks loaded: 2161111\n",
      "Columns: ['chunk_id', 'chunk_type', 'group_id', 'lang', 'text_raw']\n",
      "\n",
      "Chunk types:\n",
      "chunk_type\n",
      "case    1985178\n",
      "law      175933\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Languages:\n",
      "lang\n",
      "de         1308094\n",
      "fr          636918\n",
      "it          109091\n",
      "unknown     107008\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "chunks_path = PROCESSED_PATH / \"chunks.parquet\"\n",
    "\n",
    "if chunks_path.exists() and not CONFIG[\"force_rebuild\"]:\n",
    "    print(f\"Loading existing chunks from {chunks_path}\")\n",
    "    chunks_df = Ingestor.load_chunks(chunks_path)\n",
    "else:\n",
    "    print(\"Building chunks from CSV files...\")\n",
    "    # show_progress=True enables tqdm progress bars for loading\n",
    "    ingestor = Ingestor(DATA_PATH, show_progress=True)\n",
    "    chunks_df = ingestor.load_all()\n",
    "    \n",
    "    if CONFIG[\"sample_size\"]:\n",
    "        print(f\"Sampling {CONFIG['sample_size']} chunks for testing\")\n",
    "        chunks_df = chunks_df.sample(n=min(CONFIG[\"sample_size\"], len(chunks_df)), random_state=42)\n",
    "    \n",
    "    ingestor.save(chunks_df, chunks_path)\n",
    "    print(f\"Saved chunks to {chunks_path}\")\n",
    "\n",
    "print(f\"\\nChunks loaded: {len(chunks_df)}\")\n",
    "print(f\"Columns: {list(chunks_df.columns)}\")\n",
    "print(f\"\\nChunk types:\")\n",
    "print(chunks_df[\"chunk_type\"].value_counts())\n",
    "print(f\"\\nLanguages:\")\n",
    "print(chunks_df[\"lang\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d1d80d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample chunks:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "chunk_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "chunk_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "group_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "lang",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text_raw",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "7d922987-224b-4514-b0c6-f9349bf00dff",
       "rows": [
        [
         "0",
         "Art. 1 112",
         "law",
         "code:112",
         "de",
         "Die Einwohnergemeinde Bern tritt der Schweizerischen Eidgenossenschaft unentgeltlich als Eigentum ab:a. Das Geb√§ude des Bundesrathauses im roten Quartier der Stadt Bern, mit Nr. 229 bezeichnet, nebst den in demselben enthaltenen Einrichtungen und Mobilien, welche der Einwohnergemeinde angeh√∂ren, und unter Vorbehalt der im Artikel 62 von der Einwohnergemeinde reservierten Einrichtungen und Gegenst√§nde; b. den zwischen den Seitenfl√ºgeln des Bundesrathauses und n√∂rdlich von dem Mittelbau desselben befindlichen innern Hof von ungef√§hr 25 000 Quadratfuss Oberfl√§che. Derselbe wird abgetreten bis zu einer in Verl√§ngerung der Nordfassaden der Seitenfl√ºgel gezogenen Linie. Der in diesem Hofe befindliche Brunnen verbleibt der Einwohnergemeinde, welche denselben in gutem Zustande erhalten und ohne Genehmigung des Bundesrates an dem jetzigen baulichen Zustand mit Inbegriff der Statuen keine Ver√§nderung vornehmen soll. Sie wird den Brunnen wie bis anhin mit Wasser versehen. Die Eidgenossenschaft verpflichtet sich, den Fortbestand des Brunnens auf ihrem Eigentum als Dienstbarkeit zu √ºbernehmen. Der Brunnen sowie der Zugang zu demselben sollen dem Publikum zum angemessenen Hausgebrauch offenstehen. c. Eine Parzelle der sogenannten Vannazhalde von ungef√§hr 7280 Quadratfuss Oberfl√§che, auf welcher die Eidgenossenschaft ihr Gew√§chshaus erstellt hat. Ein Plan √ºber die abgetretene Parzelle wird der √úbereinkunft beigelegt. Die Schweizerische Eidgenossenschaft √ºbernimmt jedoch die Verpflichtung, im Falle der Erbauung einer Strasse l√§ngs der Vannazhalde den in beiliegendem Plan gelb angelegten Abschnitt dieser Parzelle der Einwohnergemeinde zum Zwecke des Strassenbaues unentgeltlich wieder abzutreten. In diesem Falle ist der Bundesrat berechtigt, im Interesse der r√§umlichen Verh√§ltnisse des Gew√§chshauses die Erstellung einer St√ºtzmauer zu verlangen, deren Kosten alsdann zur einen H√§lfte die Eidgenossenschaft und zur andern H√§lfte die Einwohnergemeinde zu tragen hat."
        ],
        [
         "1",
         "Art. 2 112",
         "law",
         "code:112",
         "de",
         "Die Einwohnergemeinde Bern wird ferner der Schweizerischen Eidgenossenschaft eine Summe von 500 000 Franken in zwischen dem Bundesrate und dem Einwohnergemeinderate zu vereinbarenden Terminen ausbezahlen. Die letzte Ratenzahlung wird jedenfalls sp√§testens auf Ende tausendachthundertsiebenundsiebenzig f√§llig."
        ],
        [
         "2",
         "Art. 3 Abs. 1 112",
         "law",
         "code:112",
         "de",
         "1 Falls die Schweizerische Eidgenossenschaft zum Zwecke der Erstellung eines von ihr zu benutzenden neuen Verwaltungsgeb√§udes einen Teil des Bauplatzes zu erwerben w√ºnscht, welcher zwischen der verl√§ngerten Bundesgasse und der neuen Promenade der kleinen Schanze, im Eigentum der Einwohnergemeinde sich befindet, so erkl√§rt sich letztere bereit, der Eidgenossenschaft den n√∂tigen Bauplatz in der verlangten Ausdehnung zum Preise von 10 Franken per Quadratfuss zu √ºbergeben, und zwar am √∂stlichen Ende des oberw√§hnten Grundeigentums der Einwohnergemeinde, oder wenn die Bem√ºhungen des Einwohnergemeinderates zur Beseitigung der Einspruchsrechte Dritter wider den Bau auf dieser Stelle erfolglos bleiben sollten, am westlichen Ende desselben. In beiden F√§llen hat sich die Abtretung auf die ganze 120 Fuss messende Tiefe des Bauplatzes zu erstrecken. Die Einwohnergemeinde ist jedoch zu einer solchen Landabtretung nur verpflichtet, wenn der Bundesrat ein daheriges Begehren innerhalb drei Monaten nach Inkrafttreten dieser √úbereinkunft an den Gemeinderat stellt."
        ],
        [
         "3",
         "Art. 3 Abs. 2 112",
         "law",
         "code:112",
         "de",
         "2 Durch Anlage des neuen Verwaltungsgeb√§udes an hier bezeichneter Stelle √ºbernimmt die Eidgenossenschaft bez√ºglich der Erstellung der Trottoirs und Trottoirsrinnen l√§ngs den Strassen, welche an das von ihr erworbene Grundeigentum grenzen, die gleichen Verpflichtungen, welche durch Artikel 5 der √úbereinkunft vom 29. Januar 1872 zwischen Staat und Gemeinde Bern den K√§ufern von Bauparzellen auf dem Territorium des n√∂rdlichen Abschnittes der kleinen Schanze √ºberbunden worden sind."
        ],
        [
         "4",
         "Art. 4 Abs. 1 112",
         "law",
         "code:112",
         "de",
         "1 Die Einwohnergemeinde Bern √ºbernimmt im fernern die Verpflichtung, auf der ihr geh√∂renden Vannazhalde keinerlei Geb√§ude zu errichten, deren Firste die jetzige H√∂he der Bundesrathaus-Terrasse √ºberragen w√ºrden."
        ],
        [
         "5",
         "Art. 4 Abs. 2 112",
         "law",
         "code:112",
         "de",
         "2 Sie √ºbernimmt auch die Verpflichtung, die erw√§hnte Terrasse zwischen dem Bundesrathause und der Vannazhalde als √∂ffentliche Anlage zu erhalten."
        ],
        [
         "6",
         "Art. 4 Abs. 3 112",
         "law",
         "code:112",
         "de",
         "3 Im Fall die Schweizerische Eidgenossenschaft von der ihr durch Artikel 3 einger√§umten Befugnis zur Beanspruchung von Land auf dem fr√ºhern Territorium der kleinen Schanze Gebrauch machen w√ºrde, so √ºbernimmt √ºberdies die Einwohnergemeinde auch dem Bunde gegen√ºber die Verpflichtung, die s√ºdlich von dem neu erstellten Verwaltungsgeb√§ude verbleibenden Teile der kleinen Schanze als √∂ffentliche Promenadenanlage zu erstellen und zu unterhalten."
        ],
        [
         "7",
         "Art. 5 Abs. 1 112",
         "law",
         "code:112",
         "de",
         "1 Sollte infolge f√∂rmlichen Beschlusses der kompetenten Beh√∂rde das Bundesrathausgeb√§ude auf h√∂ren, der Zentralverwaltung des Bundes zu dienen, so fallen die im Artikel 1 Buchstaben a, b, c bezeichneten Objekte in ihrem dannzumaligen Zustande als Eigentum an die Einwohnergemeinde Bern zur√ºck und erl√∂schen die im ersten und zweiten Absatz des Artikels 4 seitens der Einwohnergemeinde √ºbernommenen Verbindlichkeiten."
        ],
        [
         "8",
         "Art. 5 Abs. 2 112",
         "law",
         "code:112",
         "de",
         "2 F√ºr den n√§mlichen Fall √ºbernimmt die Schweizerische Eidgenossenschaft die Verpflichtung, der Einwohnergemeinde Bern die im Artikel 2 vorgesehene Summe von 500 000 Franken zur√ºckzuerstatten."
        ],
        [
         "9",
         "Art. 8 112",
         "law",
         "code:112",
         "de",
         "Infolge √úbernahme der durch diese √úbereinkunft festgesetzten Leistungen der Einwohnergemeinde Bern erkl√§rt die Schweizerische Eidgenossenschaft, dass dieselbe den ihr durch Bundesbeschl√ºsse vom 27. und 28. November 18484 auferlegten Verbindlichkeiten Gen√ºge geleistet haben soll, und entbindet die Einwohnergemeinde Bern vollst√§ndig und abschliessend von jeder weitern Verpflichtung und Inanspruchnahme f√ºr Bundessitzleistungen."
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>chunk_type</th>\n",
       "      <th>group_id</th>\n",
       "      <th>lang</th>\n",
       "      <th>text_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Art. 1 112</td>\n",
       "      <td>law</td>\n",
       "      <td>code:112</td>\n",
       "      <td>de</td>\n",
       "      <td>Die Einwohnergemeinde Bern tritt der Schweizer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Art. 2 112</td>\n",
       "      <td>law</td>\n",
       "      <td>code:112</td>\n",
       "      <td>de</td>\n",
       "      <td>Die Einwohnergemeinde Bern wird ferner der Sch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Art. 3 Abs. 1 112</td>\n",
       "      <td>law</td>\n",
       "      <td>code:112</td>\n",
       "      <td>de</td>\n",
       "      <td>1 Falls die Schweizerische Eidgenossenschaft z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Art. 3 Abs. 2 112</td>\n",
       "      <td>law</td>\n",
       "      <td>code:112</td>\n",
       "      <td>de</td>\n",
       "      <td>2 Durch Anlage des neuen Verwaltungsgeb√§udes a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Art. 4 Abs. 1 112</td>\n",
       "      <td>law</td>\n",
       "      <td>code:112</td>\n",
       "      <td>de</td>\n",
       "      <td>1 Die Einwohnergemeinde Bern √ºbernimmt im fern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Art. 4 Abs. 2 112</td>\n",
       "      <td>law</td>\n",
       "      <td>code:112</td>\n",
       "      <td>de</td>\n",
       "      <td>2 Sie √ºbernimmt auch die Verpflichtung, die er...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Art. 4 Abs. 3 112</td>\n",
       "      <td>law</td>\n",
       "      <td>code:112</td>\n",
       "      <td>de</td>\n",
       "      <td>3 Im Fall die Schweizerische Eidgenossenschaft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Art. 5 Abs. 1 112</td>\n",
       "      <td>law</td>\n",
       "      <td>code:112</td>\n",
       "      <td>de</td>\n",
       "      <td>1 Sollte infolge f√∂rmlichen Beschlusses der ko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Art. 5 Abs. 2 112</td>\n",
       "      <td>law</td>\n",
       "      <td>code:112</td>\n",
       "      <td>de</td>\n",
       "      <td>2 F√ºr den n√§mlichen Fall √ºbernimmt die Schweiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Art. 8 112</td>\n",
       "      <td>law</td>\n",
       "      <td>code:112</td>\n",
       "      <td>de</td>\n",
       "      <td>Infolge √úbernahme der durch diese √úbereinkunft...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            chunk_id chunk_type  group_id lang  \\\n",
       "0         Art. 1 112        law  code:112   de   \n",
       "1         Art. 2 112        law  code:112   de   \n",
       "2  Art. 3 Abs. 1 112        law  code:112   de   \n",
       "3  Art. 3 Abs. 2 112        law  code:112   de   \n",
       "4  Art. 4 Abs. 1 112        law  code:112   de   \n",
       "5  Art. 4 Abs. 2 112        law  code:112   de   \n",
       "6  Art. 4 Abs. 3 112        law  code:112   de   \n",
       "7  Art. 5 Abs. 1 112        law  code:112   de   \n",
       "8  Art. 5 Abs. 2 112        law  code:112   de   \n",
       "9         Art. 8 112        law  code:112   de   \n",
       "\n",
       "                                            text_raw  \n",
       "0  Die Einwohnergemeinde Bern tritt der Schweizer...  \n",
       "1  Die Einwohnergemeinde Bern wird ferner der Sch...  \n",
       "2  1 Falls die Schweizerische Eidgenossenschaft z...  \n",
       "3  2 Durch Anlage des neuen Verwaltungsgeb√§udes a...  \n",
       "4  1 Die Einwohnergemeinde Bern √ºbernimmt im fern...  \n",
       "5  2 Sie √ºbernimmt auch die Verpflichtung, die er...  \n",
       "6  3 Im Fall die Schweizerische Eidgenossenschaft...  \n",
       "7  1 Sollte infolge f√∂rmlichen Beschlusses der ko...  \n",
       "8  2 F√ºr den n√§mlichen Fall √ºbernimmt die Schweiz...  \n",
       "9  Infolge √úbernahme der durch diese √úbereinkunft...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample chunks\n",
    "print(\"Sample chunks:\")\n",
    "chunks_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c25e7ca",
   "metadata": {},
   "source": [
    "## 4. Step 2: Summarizer - Generate Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09f10861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries (mode: llm)...\n",
      "Using LLM client for summarization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d961635af1a34d85afd98100b9e26865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarizing:   0%|          | 0/21612 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(chunks_df), batch_size), desc=\u001b[33m\"\u001b[39m\u001b[33mSummarizing\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     24\u001b[39m     batch = chunks_df.iloc[i:i+batch_size]\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     batch_summaries = \u001b[43msummarizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43msummarize_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     all_summaries.append(batch_summaries)\n\u001b[32m     28\u001b[39m summaries_df = pd.concat(all_summaries, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Projects\\LEXam_kaggle\\Omnilex-Agentic-Retrieval-Competition\\src\\omnilex\\graph\\summarizer.py:197\u001b[39m, in \u001b[36mSummarizer.summarize_all\u001b[39m\u001b[34m(self, chunks_df)\u001b[39m\n\u001b[32m    195\u001b[39m     \u001b[38;5;66;03m# Generate each summary type\u001b[39;00m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m summary_type \u001b[38;5;129;01min\u001b[39;00m SummaryType:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m         summary = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummary_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m         records.append({\n\u001b[32m    199\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mchunk_id\u001b[39m\u001b[33m\"\u001b[39m: chunk_id,\n\u001b[32m    200\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msummary_type\u001b[39m\u001b[33m\"\u001b[39m: summary_type.value,\n\u001b[32m    201\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msummary_text\u001b[39m\u001b[33m\"\u001b[39m: summary.summary_text,\n\u001b[32m    202\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mentities\u001b[39m\u001b[33m\"\u001b[39m: summary.entities,\n\u001b[32m    203\u001b[39m         })\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pd.DataFrame(records)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Projects\\LEXam_kaggle\\Omnilex-Agentic-Retrieval-Competition\\src\\omnilex\\graph\\summarizer.py:210\u001b[39m, in \u001b[36mSummarizer._generate_summary\u001b[39m\u001b[34m(self, text, summary_type)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Generate a single summary.\"\"\"\u001b[39;00m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mllm\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummary_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate_heuristic(text, summary_type)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Projects\\LEXam_kaggle\\Omnilex-Agentic-Retrieval-Competition\\src\\omnilex\\graph\\summarizer.py:376\u001b[39m, in \u001b[36mSummarizer._generate_llm\u001b[39m\u001b[34m(self, text, summary_type)\u001b[39m\n\u001b[32m    350\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mLLM client not configured\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    352\u001b[39m         prompts = {\n\u001b[32m    353\u001b[39m             SummaryType.SHORT: \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mSummarize this Swiss legal text in 1-2 sentences (200-400 characters):\u001b[39m\n\u001b[32m    354\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    373\u001b[39m \u001b[33mEntities:\u001b[39m\u001b[33m\"\"\"\u001b[39m,\n\u001b[32m    374\u001b[39m         }\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m         response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m[\u001b[49m\u001b[43msummary_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m         entities = []\n\u001b[32m    379\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m summary_type == SummaryType.ENTITIES:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Projects\\LEXam_kaggle\\Omnilex-Agentic-Retrieval-Competition\\src\\omnilex\\graph\\summarizer.py:109\u001b[39m, in \u001b[36m_create_google_client.<locals>.call_llm\u001b[39m\u001b[34m(prompt)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_llm\u001b[39m(prompt: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    108\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the LLM with a prompt.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.text \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Artem Khakimov\\Desktop\\MIPT\\enterprise-rag-challenge\\.venv\\Lib\\site-packages\\google\\genai\\models.py:5227\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5225\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc > \u001b[32m0\u001b[39m:\n\u001b[32m   5226\u001b[39m   i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5227\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5228\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   5229\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5231\u001b[39m   function_map = _extra_utils.get_function_map(parsed_config)\n\u001b[32m   5232\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m function_map:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Artem Khakimov\\Desktop\\MIPT\\enterprise-rag-challenge\\.venv\\Lib\\site-packages\\google\\genai\\models.py:4009\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   4006\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   4007\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m4009\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4010\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   4011\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4013\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   4014\u001b[39m     config, \u001b[33m'\u001b[39m\u001b[33mshould_return_http_response\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4015\u001b[39m ):\n\u001b[32m   4016\u001b[39m   return_value = types.GenerateContentResponse(sdk_http_response=response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Artem Khakimov\\Desktop\\MIPT\\enterprise-rag-challenge\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py:1386\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m   1377\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1378\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1381\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1382\u001b[39m ) -> SdkHttpResponse:\n\u001b[32m   1383\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1384\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m   1385\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m   response_body = (\n\u001b[32m   1388\u001b[39m       response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1389\u001b[39m   )\n\u001b[32m   1390\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m SdkHttpResponse(headers=response.headers, body=response_body)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Artem Khakimov\\Desktop\\MIPT\\enterprise-rag-challenge\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py:1222\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1219\u001b[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001b[32m   1220\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Artem Khakimov\\Desktop\\MIPT\\enterprise-rag-challenge\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:475\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    473\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Artem Khakimov\\Desktop\\MIPT\\enterprise-rag-challenge\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:376\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    374\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Artem Khakimov\\Desktop\\MIPT\\enterprise-rag-challenge\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:398\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    399\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Artem Khakimov\\Desktop\\MIPT\\enterprise-rag-challenge\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:478\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    480\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Artem Khakimov\\Desktop\\MIPT\\enterprise-rag-challenge\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py:1192\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1188\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m   1189\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m   1190\u001b[39m   )\n\u001b[32m   1191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_httpx_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m      \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m      \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1199\u001b[39m   errors.APIError.raise_for_response(response)\n\u001b[32m   1200\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m   1201\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m   1202\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Artem Khakimov\\Desktop\\MIPT\\enterprise-rag-challenge\\.venv\\Lib\\site-packages\\httpx\\_client.py:837\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    812\u001b[39m     request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m         method=method,\n\u001b[32m    814\u001b[39m         url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m         extensions=extensions,\n\u001b[32m    824\u001b[39m     )\n\u001b[32m    825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.send(request, auth=auth, follow_redirects=follow_redirects)\n\u001b[32m    827\u001b[39m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstream\u001b[39m(\n\u001b[32m    829\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    830\u001b[39m     method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    831\u001b[39m     url: URL | \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    832\u001b[39m     *,\n\u001b[32m    833\u001b[39m     content: RequestContent | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    834\u001b[39m     data: RequestData | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    835\u001b[39m     files: RequestFiles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    836\u001b[39m     json: typing.Any | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m--> \u001b[39m\u001b[32m837\u001b[39m     params: QueryParamTypes | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    838\u001b[39m     headers: HeaderTypes | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    839\u001b[39m     cookies: CookieTypes | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    840\u001b[39m     auth: AuthTypes | UseClientDefault | \u001b[38;5;28;01mNone\u001b[39;00m = USE_CLIENT_DEFAULT,\n\u001b[32m    841\u001b[39m     follow_redirects: \u001b[38;5;28mbool\u001b[39m | UseClientDefault = USE_CLIENT_DEFAULT,\n\u001b[32m    842\u001b[39m     timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,\n\u001b[32m    843\u001b[39m     extensions: RequestExtensions | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    844\u001b[39m ) -> typing.Iterator[Response]:\n\u001b[32m    845\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    846\u001b[39m \u001b[33;03m    Alternative to `httpx.request()` that streams the response body\u001b[39;00m\n\u001b[32m    847\u001b[39m \u001b[33;03m    instead of loading it into memory at once.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    853\u001b[39m \u001b[33;03m    [0]: /quickstart#streaming-responses\u001b[39;00m\n\u001b[32m    854\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    855\u001b[39m     request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    856\u001b[39m         method=method,\n\u001b[32m    857\u001b[39m         url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    866\u001b[39m         extensions=extensions,\n\u001b[32m    867\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Artem Khakimov\\Desktop\\MIPT\\enterprise-rag-challenge\\.venv\\Lib\\site-packages\\httpx\\_client.py:926\u001b[39m, in \u001b[36msend\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    922\u001b[39m         response.read()\n\u001b[32m    924\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    927\u001b[39m     response.close()\n\u001b[32m    928\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Artem Khakimov\\Desktop\\MIPT\\enterprise-rag-challenge\\.venv\\Lib\\site-packages\\httpx\\_client.py:954\u001b[39m, in \u001b[36m_send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    951\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m    953\u001b[39m response.history = \u001b[38;5;28mlist\u001b[39m(history)\n\u001b[32m--> \u001b[39m\u001b[32m954\u001b[39m response.read()\n\u001b[32m    955\u001b[39m request = next_request\n\u001b[32m    956\u001b[39m history.append(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Artem Khakimov\\Desktop\\MIPT\\enterprise-rag-challenge\\.venv\\Lib\\site-packages\\httpx\\_client.py:991\u001b[39m, in \u001b[36m_send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    988\u001b[39m request = \u001b[38;5;28mself\u001b[39m._build_redirect_request(request, response)\n\u001b[32m    989\u001b[39m history = history + [response]\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_redirects:\n\u001b[32m    992\u001b[39m     response.read()\n\u001b[32m    993\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Artem Khakimov\\Desktop\\MIPT\\enterprise-rag-challenge\\.venv\\Lib\\site-packages\\httpx\\_client.py:1027\u001b[39m, in \u001b[36m_send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1022\u001b[39m \u001b[38;5;28mself\u001b[39m.cookies.extract_cookies(response)\n\u001b[32m   1023\u001b[39m response.default_encoding = \u001b[38;5;28mself\u001b[39m._default_encoding\n\u001b[32m   1025\u001b[39m logger.info(\n\u001b[32m   1026\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mHTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1027\u001b[39m     request.method,\n\u001b[32m   1028\u001b[39m     request.url,\n\u001b[32m   1029\u001b[39m     response.http_version,\n\u001b[32m   1030\u001b[39m     response.status_code,\n\u001b[32m   1031\u001b[39m     response.reason_phrase,\n\u001b[32m   1032\u001b[39m )\n\u001b[32m   1034\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Artem Khakimov\\Desktop\\MIPT\\enterprise-rag-challenge\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:236\u001b[39m, in \u001b[36mhandle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Artem Khakimov\\Desktop\\MIPT\\enterprise-rag-challenge\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Artem Khakimov\\Desktop\\MIPT\\enterprise-rag-challenge\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Artem Khakimov\\Desktop\\MIPT\\enterprise-rag-challenge\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http_proxy.py:343\u001b[39m, in \u001b[36mTunnelHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    336\u001b[39m             \u001b[38;5;28mself\u001b[39m._connection = HTTP11Connection(\n\u001b[32m    337\u001b[39m                 origin=\u001b[38;5;28mself\u001b[39m._remote_origin,\n\u001b[32m    338\u001b[39m                 stream=stream,\n\u001b[32m    339\u001b[39m                 keepalive_expiry=\u001b[38;5;28mself\u001b[39m._keepalive_expiry,\n\u001b[32m    340\u001b[39m             )\n\u001b[32m    342\u001b[39m         \u001b[38;5;28mself\u001b[39m._connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Artem Khakimov\\Desktop\\MIPT\\enterprise-rag-challenge\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Artem Khakimov\\Desktop\\MIPT\\enterprise-rag-challenge\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Artem Khakimov\\Desktop\\MIPT\\enterprise-rag-challenge\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Artem Khakimov\\Desktop\\MIPT\\enterprise-rag-challenge\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Artem Khakimov\\Desktop\\MIPT\\enterprise-rag-challenge\\.venv\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1295\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1291\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1292\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1293\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1294\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1168\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1166\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1167\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "summaries_path = PROCESSED_PATH / \"summaries.parquet\"\n",
    "\n",
    "if summaries_path.exists() and not CONFIG[\"force_rebuild\"]:\n",
    "    print(f\"Loading existing summaries from {summaries_path}\")\n",
    "    summaries_df = Summarizer.load_summaries(summaries_path)\n",
    "else:\n",
    "    print(f\"Generating summaries (mode: {CONFIG['summarizer_mode']})...\")\n",
    "    \n",
    "    # Create summarizer - use llm_client if mode is \"llm\"\n",
    "    if CONFIG[\"summarizer_mode\"] == \"llm\":\n",
    "        # llm_client was created in imports cell\n",
    "        summarizer = Summarizer(mode=\"llm\", llm_client=llm_client)\n",
    "        print(f\"Using LLM client for summarization\")\n",
    "    else:\n",
    "        summarizer = Summarizer(mode=\"heuristic\")\n",
    "        print(\"Using heuristic summarization\")\n",
    "    \n",
    "    # Process in batches for progress tracking\n",
    "    # Smaller batches for LLM mode (API rate limits)\n",
    "    batch_size = 100 if CONFIG[\"summarizer_mode\"] == \"llm\" else 1000\n",
    "    all_summaries = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(chunks_df), batch_size), desc=\"Summarizing\"):\n",
    "        batch = chunks_df.iloc[i:i+batch_size]\n",
    "        batch_summaries = summarizer.summarize_all(batch)\n",
    "        all_summaries.append(batch_summaries)\n",
    "    \n",
    "    summaries_df = pd.concat(all_summaries, ignore_index=True)\n",
    "    summarizer.save(summaries_df, summaries_path)\n",
    "    print(f\"Saved summaries to {summaries_path}\")\n",
    "\n",
    "print(f\"\\nSummaries generated: {len(summaries_df)}\")\n",
    "print(f\"Summary types:\")\n",
    "print(summaries_df[\"summary_type\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce1008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample summaries for one chunk\n",
    "sample_chunk_id = chunks_df[\"chunk_id\"].iloc[0]\n",
    "print(f\"Summaries for: {sample_chunk_id}\\n\")\n",
    "\n",
    "for _, row in summaries_df[summaries_df[\"chunk_id\"] == sample_chunk_id].iterrows():\n",
    "    print(f\"--- {row['summary_type'].upper()} ---\")\n",
    "    print(row[\"summary_text\"][:500])\n",
    "    if row[\"entities\"]:\n",
    "        print(f\"Entities: {row['entities'][:10]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55be7d69",
   "metadata": {},
   "source": [
    "## 5. Step 3: Embedder - Build Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08170067",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dir = PROCESSED_PATH / \"embeddings\"\n",
    "embeddings_path = embeddings_dir / \"embeddings.npy\"\n",
    "\n",
    "if embeddings_path.exists() and not CONFIG[\"force_rebuild\"]:\n",
    "    print(f\"Loading existing embedder from {embeddings_dir}\")\n",
    "    embedder = Embedder.load(embeddings_dir, model_name=CONFIG[\"embedding_model\"])\n",
    "else:\n",
    "    print(f\"Building embeddings with {CONFIG['embedding_model']}...\")\n",
    "    embedder = Embedder(model_name=CONFIG[\"embedding_model\"])\n",
    "    \n",
    "    # Build embeddings from retrieval summaries\n",
    "    embeddings = embedder.build_embeddings(\n",
    "        summaries_df,\n",
    "        summary_type=\"retrieval\",\n",
    "        batch_size=CONFIG[\"embedding_batch_size\"],\n",
    "        show_progress=True,\n",
    "    )\n",
    "    print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "    \n",
    "    # Build FAISS index\n",
    "    print(f\"Building FAISS index (type: {CONFIG['faiss_index_type']})...\")\n",
    "    embedder.build_faiss_index(embeddings, index_type=CONFIG[\"faiss_index_type\"])\n",
    "    \n",
    "    # Build BM25 index\n",
    "    print(\"Building BM25 index...\")\n",
    "    embedder.build_bm25_index(summaries_df, summary_type=\"retrieval\")\n",
    "    \n",
    "    # Save\n",
    "    embedder.save(embeddings_dir)\n",
    "    print(f\"Saved embedder to {embeddings_dir}\")\n",
    "\n",
    "print(f\"\\nIndexed chunks: {len(embedder.get_chunk_ids())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268593c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test search\n",
    "test_query = \"What are the requirements for a valid contract under Swiss law?\"\n",
    "\n",
    "print(f\"Test query: {test_query}\\n\")\n",
    "\n",
    "print(\"=== Vector Search ===\")\n",
    "vec_results = embedder.search_vector(test_query, top_k=5)\n",
    "for chunk_id, score in vec_results:\n",
    "    print(f\"  {score:.4f} | {chunk_id}\")\n",
    "\n",
    "print(\"\\n=== BM25 Search ===\")\n",
    "bm25_results = embedder.search_bm25(test_query, top_k=5)\n",
    "for chunk_id, score in bm25_results:\n",
    "    print(f\"  {score:.4f} | {chunk_id}\")\n",
    "\n",
    "print(\"\\n=== Hybrid Search (RRF) ===\")\n",
    "hybrid_results = embedder.search_hybrid(test_query, top_k=5)\n",
    "for chunk_id, score in hybrid_results:\n",
    "    print(f\"  {score:.4f} | {chunk_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c52513",
   "metadata": {},
   "source": [
    "## 6. Step 4: GraphBuilder - Build Graph Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437a4043",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dir = PROCESSED_PATH / \"graph\"\n",
    "edges_similar_path = graph_dir / \"edges_similar.parquet\"\n",
    "\n",
    "if edges_similar_path.exists() and not CONFIG[\"force_rebuild\"]:\n",
    "    print(f\"Loading existing graph from {graph_dir}\")\n",
    "    graph_builder = GraphBuilder.load(graph_dir)\n",
    "else:\n",
    "    print(\"Building graph edges...\")\n",
    "    graph_builder = GraphBuilder()\n",
    "    \n",
    "    # Build SIMILAR_TO edges from embeddings\n",
    "    if embedder._embeddings is not None:\n",
    "        print(\"Building SIMILAR_TO edges...\")\n",
    "        similar_df = graph_builder.build_similar_edges(\n",
    "            embedder._embeddings,\n",
    "            embedder.get_chunk_ids(),\n",
    "            k=CONFIG[\"similar_k\"],\n",
    "            min_cos=CONFIG[\"similar_min_cos\"],\n",
    "        )\n",
    "        print(f\"  SIMILAR_TO edges: {len(similar_df)}\")\n",
    "    \n",
    "    # Build CO_CITED_WITH edges from training data\n",
    "    train_path = DATA_PATH / \"train.csv\"\n",
    "    if train_path.exists():\n",
    "        print(\"Building CO_CITED_WITH edges...\")\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        cocite_df = graph_builder.build_cocite_edges(\n",
    "            train_df,\n",
    "            top_m=CONFIG[\"cocite_top_m\"],\n",
    "        )\n",
    "        print(f\"  CO_CITED_WITH edges: {len(cocite_df)}\")\n",
    "    \n",
    "    # Build DocGroup mapping\n",
    "    print(\"Building DocGroup mapping...\")\n",
    "    groups_df, chunk_to_group_df = graph_builder.build_groups(chunks_df)\n",
    "    print(f\"  Groups: {len(groups_df)}\")\n",
    "    print(f\"  Chunk-to-group mappings: {len(chunk_to_group_df)}\")\n",
    "    \n",
    "    # Save\n",
    "    graph_builder.save(graph_dir)\n",
    "    print(f\"Saved graph to {graph_dir}\")\n",
    "\n",
    "print(f\"\\nGraph statistics:\")\n",
    "print(f\"  SIMILAR_TO edges: {sum(len(v) for v in graph_builder.similar_edges.values())}\")\n",
    "print(f\"  CO_CITED_WITH edges: {sum(len(v) for v in graph_builder.cocite_edges.values())}\")\n",
    "print(f\"  Groups: {len(graph_builder.group_to_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a5f7b3",
   "metadata": {},
   "source": [
    "## 7. Full Inference Pipeline (–≠—Ç–∞–ø 1)\n",
    "\n",
    "Pipeline:\n",
    "1. **Query preprocessing** (optional LLM summary)\n",
    "2. **Initial retrieval** (BM25 + FAISS ‚Üí RRF fusion)\n",
    "3. **Graph expansion** (SIMILAR_TO, CO_CITED_WITH, PART_OF)\n",
    "4. **Fast scoring** (linear combination: Œ±¬∑retr + Œ≤¬∑sim + Œ≥¬∑cocite + Œ¥¬∑group)\n",
    "5. **LLM Rerank** (optional, on top K candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9791491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Create LLM Reranker (for Etap 1 inference)\n",
    "# =============================================================================\n",
    "\n",
    "reranker_config = RerankerConfig(\n",
    "    top_k_to_rerank=CONFIG[\"top_k_to_rerank\"],\n",
    "    top_k_final=CONFIG[\"top_k_final\"],\n",
    "    batch_size=CONFIG[\"rerank_batch_size\"],\n",
    "    use_llm=CONFIG[\"use_llm_rerank\"],\n",
    "    relevance_threshold=CONFIG[\"relevance_threshold\"],\n",
    "    max_text_length=300,\n",
    ")\n",
    "\n",
    "# Use llm_client created earlier (Google Gemini or OpenAI)\n",
    "reranker = LLMReranker(\n",
    "    llm_client=llm_client if CONFIG[\"use_llm_rerank\"] else None,\n",
    "    config=reranker_config,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ LLM Reranker created:\")\n",
    "print(f\"   use_llm: {reranker_config.use_llm}\")\n",
    "print(f\"   top_k_to_rerank: {reranker_config.top_k_to_rerank}\")\n",
    "print(f\"   batch_size: {reranker_config.batch_size}\")\n",
    "print(f\"   relevance_threshold: {reranker_config.relevance_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2808b0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(\n",
    "    query: str,\n",
    "    embedder: Embedder,\n",
    "    graph_builder: GraphBuilder,\n",
    "    config: dict,\n",
    "    reranker: LLMReranker | None = None,\n",
    "    chunks_df: pd.DataFrame | None = None,\n",
    "    summaries_df: pd.DataFrame | None = None,\n",
    "    verbose: bool = True,\n",
    ") -> list[tuple[str, float, dict]]:\n",
    "    \"\"\"\n",
    "    Run full inference pipeline for a single query.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Query preprocessing (optional LLM summary)\n",
    "    2. Initial retrieval (BM25 + FAISS ‚Üí RRF)\n",
    "    3. Graph expansion (SIMILAR_TO, CO_CITED_WITH, PART_OF)\n",
    "    4. Fast scoring (linear combination)\n",
    "    5. LLM Rerank (optional, on top K candidates)\n",
    "    \n",
    "    Returns:\n",
    "        List of (chunk_id, score, features) tuples\n",
    "    \"\"\"\n",
    "    # Step 0: Query preprocessing\n",
    "    processed_query = query\n",
    "    if config.get(\"use_query_summary\") and reranker is not None:\n",
    "        processed_query = reranker.summarize_query(query)\n",
    "        if verbose:\n",
    "            print(f\"Query summary: {processed_query[:100]}...\")\n",
    "    \n",
    "    # Step 1: Initial retrieval (hybrid search)\n",
    "    if verbose:\n",
    "        print(f\"Query: {query[:80]}{'...' if len(query) > 80 else ''}\\n\")\n",
    "        print(\"Step 1: Initial retrieval (BM25 + FAISS ‚Üí RRF)...\")\n",
    "    \n",
    "    initial_results = embedder.search_hybrid(\n",
    "        processed_query,\n",
    "        top_k=config[\"top_k_retrieval\"],\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  ‚Üí {len(initial_results)} initial candidates\")\n",
    "        for cid, score in initial_results[:3]:\n",
    "            print(f\"    {score:.4f} | {cid[:60]}\")\n",
    "        if len(initial_results) > 3:\n",
    "            print(f\"    ... and {len(initial_results) - 3} more\")\n",
    "    \n",
    "    # Step 2: Graph expansion\n",
    "    if verbose:\n",
    "        print(\"\\nStep 2: Graph expansion...\")\n",
    "    \n",
    "    expansion_params = ExpansionParams(\n",
    "        k_expand_sim=config[\"k_expand_sim\"],\n",
    "        k_expand_cocite=config[\"k_expand_cocite\"],\n",
    "        k_expand_siblings=config[\"k_expand_siblings\"],\n",
    "        max_candidates=config[\"max_candidates\"],\n",
    "    )\n",
    "    \n",
    "    expanded = graph_builder.expand_candidates(initial_results, expansion_params)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  ‚Üí {len(expanded)} candidates after expansion\")\n",
    "        # Count by expansion reason\n",
    "        reasons = {}\n",
    "        for _, _, reason in expanded:\n",
    "            key = reason.split(\"(\")[0].strip()\n",
    "            reasons[key] = reasons.get(key, 0) + 1\n",
    "        for reason, count in sorted(reasons.items(), key=lambda x: -x[1]):\n",
    "            print(f\"    {reason}: {count}\")\n",
    "    \n",
    "    # Step 3: Fast scoring (linear combination)\n",
    "    if verbose:\n",
    "        print(\"\\nStep 3: Fast scoring...\")\n",
    "    \n",
    "    scoring_params = ScoringParams(\n",
    "        alpha=config[\"alpha\"],\n",
    "        beta=config[\"beta\"],\n",
    "        gamma=config[\"gamma\"],\n",
    "        delta=config[\"delta\"],\n",
    "    )\n",
    "    \n",
    "    initial_set = set(cid for cid, _ in initial_results)\n",
    "    scored = graph_builder.score_candidates(expanded, initial_set, scoring_params)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  ‚Üí Top scores: {[f'{s[1]:.3f}' for s in scored[:5]]}\")\n",
    "    \n",
    "    # Step 4: LLM Rerank (optional)\n",
    "    if config.get(\"use_llm_rerank\") and reranker is not None:\n",
    "        if verbose:\n",
    "            print(f\"\\nStep 4: LLM Rerank (top {config['top_k_to_rerank']} candidates)...\")\n",
    "        \n",
    "        final_results = reranker.rerank(\n",
    "            query=query,\n",
    "            candidates=scored[:config[\"top_k_to_rerank\"]],\n",
    "            chunks_df=chunks_df,\n",
    "            summaries_df=summaries_df,\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  ‚Üí {len(final_results)} after rerank + threshold filter\")\n",
    "    else:\n",
    "        # No LLM rerank: just take top K\n",
    "        final_results = scored[:config[\"top_k_final\"]]\n",
    "    \n",
    "    # Step 5: Final output\n",
    "    final_results = final_results[:config[\"top_k_final\"]]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"FINAL: Top {len(final_results)} results:\")\n",
    "        print(f\"{'='*60}\")\n",
    "        for i, (cid, score, features) in enumerate(final_results[:10], 1):\n",
    "            llm_info = f\" llm={features.get('llm_score', '-'):.2f}\" if 'llm_score' in features else \"\"\n",
    "            print(f\"  {i:2d}. [{score:.3f}] {cid[:55]}...{llm_info}\")\n",
    "        if len(final_results) > 10:\n",
    "            print(f\"  ... and {len(final_results) - 10} more\")\n",
    "    \n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784e5212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference on a sample query\n",
    "test_query = \"What are the requirements for a valid contract under Swiss law?\"\n",
    "\n",
    "print(\"üîç Running full inference pipeline...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = run_inference(\n",
    "    query=test_query, \n",
    "    embedder=embedder, \n",
    "    graph_builder=graph_builder, \n",
    "    config=CONFIG,\n",
    "    reranker=reranker,\n",
    "    chunks_df=chunks_df,\n",
    "    summaries_df=summaries_df,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78faa524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format as submission\n",
    "def format_predictions(results: list[tuple[str, float, dict]]) -> str:\n",
    "    \"\"\"Format results as semicolon-separated citations.\"\"\"\n",
    "    return \";\".join([chunk_id for chunk_id, _, _ in results])\n",
    "\n",
    "prediction = format_predictions(results)\n",
    "print(f\"Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16ccdd6",
   "metadata": {},
   "source": [
    "## 8. Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6f35f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation set\n",
    "val_path = DATA_PATH / \"val.csv\"\n",
    "\n",
    "if val_path.exists():\n",
    "    val_df = pd.read_csv(val_path)\n",
    "    print(f\"Validation set: {len(val_df)} queries\")\n",
    "    display(val_df.head())\n",
    "else:\n",
    "    print(f\"Validation file not found: {val_path}\")\n",
    "    val_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fdfee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if val_df is not None:\n",
    "    from omnilex.evaluation.metrics import citation_f1, macro_f1\n",
    "    \n",
    "    predictions = []\n",
    "    gold_list = []\n",
    "    \n",
    "    print(f\"Running inference on {len(val_df)} validation queries...\")\n",
    "    print(f\"LLM Rerank: {'ON' if CONFIG['use_llm_rerank'] else 'OFF'}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for idx, row in tqdm(val_df.iterrows(), total=len(val_df), desc=\"Inference\"):\n",
    "        query = row[\"query\"]\n",
    "        gold = row.get(\"gold_citations\", \"\")\n",
    "        \n",
    "        # Run inference with reranker\n",
    "        results = run_inference(\n",
    "            query=query, \n",
    "            embedder=embedder, \n",
    "            graph_builder=graph_builder, \n",
    "            config=CONFIG,\n",
    "            reranker=reranker,\n",
    "            chunks_df=chunks_df,\n",
    "            summaries_df=summaries_df,\n",
    "            verbose=False,\n",
    "        )\n",
    "        \n",
    "        # Format prediction\n",
    "        pred_citations = [chunk_id for chunk_id, _, _ in results]\n",
    "        gold_citations = [c.strip() for c in str(gold).split(\";\") if c.strip()]\n",
    "        \n",
    "        predictions.append(pred_citations)\n",
    "        gold_list.append(gold_citations)\n",
    "        \n",
    "        # Per-query metrics\n",
    "        metrics = citation_f1(pred_citations, gold_citations)\n",
    "        print(f\"\\n[{idx+1}] Query: {query[:60]}...\")\n",
    "        print(f\"    Pred: {len(pred_citations)} | Gold: {len(gold_citations)}\")\n",
    "        print(f\"    P={metrics['precision']:.3f} R={metrics['recall']:.3f} F1={metrics['f1']:.3f}\")\n",
    "        \n",
    "        # Show matches\n",
    "        matches = set(pred_citations) & set(gold_citations)\n",
    "        if matches:\n",
    "            print(f\"    ‚úÖ Matches: {list(matches)[:3]}{'...' if len(matches) > 3 else ''}\")\n",
    "    \n",
    "    # Overall metrics\n",
    "    overall = macro_f1(predictions, gold_list)\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä OVERALL MACRO METRICS:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Precision: {overall['macro_precision']:.4f}\")\n",
    "    print(f\"  Recall:    {overall['macro_recall']:.4f}\")\n",
    "    print(f\"  F1:        {overall['macro_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c09d8b4",
   "metadata": {},
   "source": [
    "## 9. Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae90af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set and generate submission\n",
    "test_path = DATA_PATH / \"test.csv\"\n",
    "\n",
    "if test_path.exists():\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    print(f\"üìù Test set: {len(test_df)} queries\")\n",
    "    print(f\"   LLM Rerank: {'ON' if CONFIG['use_llm_rerank'] else 'OFF'}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    submission_records = []\n",
    "    \n",
    "    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Generating submission\"):\n",
    "        query_id = row[\"query_id\"]\n",
    "        query = row[\"query\"]\n",
    "        \n",
    "        results = run_inference(\n",
    "            query=query, \n",
    "            embedder=embedder, \n",
    "            graph_builder=graph_builder, \n",
    "            config=CONFIG,\n",
    "            reranker=reranker,\n",
    "            chunks_df=chunks_df,\n",
    "            summaries_df=summaries_df,\n",
    "            verbose=False,\n",
    "        )\n",
    "        \n",
    "        prediction = format_predictions(results)\n",
    "        \n",
    "        submission_records.append({\n",
    "            \"query_id\": query_id,\n",
    "            \"predicted_citations\": prediction,\n",
    "        })\n",
    "    \n",
    "    submission_df = pd.DataFrame(submission_records)\n",
    "    \n",
    "    # Save submission\n",
    "    submission_path = OUTPUT_PATH / \"submission.csv\"\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    print(f\"\\n‚úÖ Submission saved to: {submission_path}\")\n",
    "    print(f\"   Total queries: {len(submission_df)}\")\n",
    "    \n",
    "    display(submission_df.head())\n",
    "else:\n",
    "    print(f\"‚ùå Test file not found: {test_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd23429e",
   "metadata": {},
   "source": [
    "## 10. Debug: Inspect Graph Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eaf1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_chunk(chunk_id: str, graph_builder: GraphBuilder, chunks_df: pd.DataFrame):\n",
    "    \"\"\"Inspect a chunk's graph neighborhood.\"\"\"\n",
    "    print(f\"=== Chunk: {chunk_id} ===\")\n",
    "    \n",
    "    # Basic info\n",
    "    chunk_row = chunks_df[chunks_df[\"chunk_id\"] == chunk_id]\n",
    "    if not chunk_row.empty:\n",
    "        row = chunk_row.iloc[0]\n",
    "        print(f\"Type: {row['chunk_type']}\")\n",
    "        print(f\"Group: {row['group_id']}\")\n",
    "        print(f\"Lang: {row['lang']}\")\n",
    "        print(f\"Text: {row['text_raw'][:200]}...\")\n",
    "    \n",
    "    # Similar neighbors\n",
    "    print(f\"\\nSIMILAR_TO neighbors:\")\n",
    "    neighbors = graph_builder.similar_edges.get(chunk_id, [])\n",
    "    for n_id, score in neighbors[:5]:\n",
    "        print(f\"  {score:.4f} | {n_id}\")\n",
    "    if len(neighbors) > 5:\n",
    "        print(f\"  ... and {len(neighbors) - 5} more\")\n",
    "    \n",
    "    # Co-cited neighbors\n",
    "    print(f\"\\nCO_CITED_WITH neighbors:\")\n",
    "    neighbors = graph_builder.cocite_edges.get(chunk_id, [])\n",
    "    for n_id, weight in neighbors[:5]:\n",
    "        print(f\"  {weight:.4f} | {n_id}\")\n",
    "    if len(neighbors) > 5:\n",
    "        print(f\"  ... and {len(neighbors) - 5} more\")\n",
    "    \n",
    "    # Group siblings\n",
    "    group_id = graph_builder.chunk_to_group.get(chunk_id)\n",
    "    if group_id:\n",
    "        print(f\"\\nPART_OF group: {group_id}\")\n",
    "        siblings = graph_builder.group_to_chunks.get(group_id, [])\n",
    "        print(f\"Siblings ({len(siblings)} total):\")\n",
    "        for sib in siblings[:5]:\n",
    "            if sib != chunk_id:\n",
    "                print(f\"  {sib}\")\n",
    "\n",
    "# Example usage\n",
    "if len(chunks_df) > 0:\n",
    "    sample_id = chunks_df[\"chunk_id\"].iloc[0]\n",
    "    inspect_chunk(sample_id, graph_builder, chunks_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c1c62",
   "metadata": {},
   "source": [
    "## 11. Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e820b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_config(\n",
    "    config: dict, \n",
    "    val_df: pd.DataFrame,\n",
    "    embedder: Embedder,\n",
    "    graph_builder: GraphBuilder,\n",
    "    reranker: LLMReranker | None = None,\n",
    "    chunks_df: pd.DataFrame | None = None,\n",
    "    summaries_df: pd.DataFrame | None = None,\n",
    ") -> dict:\n",
    "    \"\"\"Evaluate a configuration on validation set.\"\"\"\n",
    "    from omnilex.evaluation.metrics import macro_f1\n",
    "    \n",
    "    predictions = []\n",
    "    gold_list = []\n",
    "    \n",
    "    for _, row in val_df.iterrows():\n",
    "        query = row[\"query\"]\n",
    "        gold = row.get(\"gold_citations\", \"\")\n",
    "        \n",
    "        results = run_inference(\n",
    "            query=query,\n",
    "            embedder=embedder,\n",
    "            graph_builder=graph_builder,\n",
    "            config=config,\n",
    "            reranker=reranker,\n",
    "            chunks_df=chunks_df,\n",
    "            summaries_df=summaries_df,\n",
    "            verbose=False,\n",
    "        )\n",
    "        \n",
    "        pred_citations = [chunk_id for chunk_id, _, _ in results]\n",
    "        gold_citations = [c.strip() for c in str(gold).split(\";\") if c.strip()]\n",
    "        \n",
    "        predictions.append(pred_citations)\n",
    "        gold_list.append(gold_citations)\n",
    "    \n",
    "    return macro_f1(predictions, gold_list)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Grid Search: Scoring Weights (Œ±, Œ≤, Œ≥, Œ¥)\n",
    "# =============================================================================\n",
    "\n",
    "if val_df is not None and len(val_df) <= 20:  # Only for small val set\n",
    "    print(\"üîß Grid search over scoring weights...\")\n",
    "    print(\"   (LLM Rerank OFF for speed)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    best_f1 = 0\n",
    "    best_params = None\n",
    "    results_table = []\n",
    "    \n",
    "    # Temporarily disable LLM rerank for faster grid search\n",
    "    test_reranker = LLMReranker(llm_client=None, config=RerankerConfig(use_llm=False))\n",
    "    \n",
    "    for alpha in [0.5, 1.0, 1.5]:\n",
    "        for beta in [0.3, 0.6, 0.9]:\n",
    "            for gamma in [0.4, 0.8, 1.2]:\n",
    "                test_config = CONFIG.copy()\n",
    "                test_config[\"alpha\"] = alpha\n",
    "                test_config[\"beta\"] = beta\n",
    "                test_config[\"gamma\"] = gamma\n",
    "                test_config[\"use_llm_rerank\"] = False\n",
    "                \n",
    "                metrics = evaluate_config(\n",
    "                    test_config, val_df, embedder, graph_builder,\n",
    "                    reranker=test_reranker,\n",
    "                    chunks_df=chunks_df,\n",
    "                    summaries_df=summaries_df,\n",
    "                )\n",
    "                f1 = metrics[\"macro_f1\"]\n",
    "                \n",
    "                results_table.append({\n",
    "                    \"Œ±\": alpha, \"Œ≤\": beta, \"Œ≥\": gamma, \n",
    "                    \"F1\": f1, \"P\": metrics[\"macro_precision\"], \"R\": metrics[\"macro_recall\"]\n",
    "                })\n",
    "                \n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_params = (alpha, beta, gamma)\n",
    "                    print(f\"  ‚ú® NEW BEST: Œ±={alpha}, Œ≤={beta}, Œ≥={gamma} ‚Üí F1={f1:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"üèÜ BEST: Œ±={best_params[0]}, Œ≤={best_params[1]}, Œ≥={best_params[2]}\")\n",
    "    print(f\"   Macro F1 = {best_f1:.4f}\")\n",
    "    \n",
    "    # Show top 5 configs\n",
    "    results_sorted = sorted(results_table, key=lambda x: -x[\"F1\"])[:5]\n",
    "    print(\"\\nTop 5 configurations:\")\n",
    "    for r in results_sorted:\n",
    "        print(f\"  Œ±={r['Œ±']}, Œ≤={r['Œ≤']}, Œ≥={r['Œ≥']} ‚Üí F1={r['F1']:.4f} (P={r['P']:.3f}, R={r['R']:.3f})\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping grid search (val set too large or missing)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d3e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Ablation Study: With vs Without LLM Rerank\n",
    "# =============================================================================\n",
    "\n",
    "if val_df is not None and len(val_df) <= 20:\n",
    "    print(\"üî¨ Ablation Study: LLM Rerank Impact\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test WITHOUT LLM Rerank\n",
    "    print(\"\\n1Ô∏è‚É£ Without LLM Rerank (fast scoring only):\")\n",
    "    config_no_llm = CONFIG.copy()\n",
    "    config_no_llm[\"use_llm_rerank\"] = False\n",
    "    \n",
    "    no_llm_reranker = LLMReranker(llm_client=None, config=RerankerConfig(use_llm=False))\n",
    "    metrics_no_llm = evaluate_config(\n",
    "        config_no_llm, val_df, embedder, graph_builder,\n",
    "        reranker=no_llm_reranker,\n",
    "        chunks_df=chunks_df,\n",
    "        summaries_df=summaries_df,\n",
    "    )\n",
    "    print(f\"   P={metrics_no_llm['macro_precision']:.4f} R={metrics_no_llm['macro_recall']:.4f} F1={metrics_no_llm['macro_f1']:.4f}\")\n",
    "    \n",
    "    # Test WITH LLM Rerank\n",
    "    print(\"\\n2Ô∏è‚É£ With LLM Rerank:\")\n",
    "    config_with_llm = CONFIG.copy()\n",
    "    config_with_llm[\"use_llm_rerank\"] = True\n",
    "    \n",
    "    metrics_with_llm = evaluate_config(\n",
    "        config_with_llm, val_df, embedder, graph_builder,\n",
    "        reranker=reranker,\n",
    "        chunks_df=chunks_df,\n",
    "        summaries_df=summaries_df,\n",
    "    )\n",
    "    print(f\"   P={metrics_with_llm['macro_precision']:.4f} R={metrics_with_llm['macro_recall']:.4f} F1={metrics_with_llm['macro_f1']:.4f}\")\n",
    "    \n",
    "    # Delta\n",
    "    delta_f1 = metrics_with_llm['macro_f1'] - metrics_no_llm['macro_f1']\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"üìä LLM Rerank Impact: ŒîF1 = {delta_f1:+.4f}\")\n",
    "    if delta_f1 > 0:\n",
    "        print(f\"   ‚úÖ LLM Rerank improves F1 by {delta_f1*100:.1f}%\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è LLM Rerank decreases F1 by {abs(delta_f1)*100:.1f}%\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping ablation study (val set too large or missing)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python RAG env",
   "language": "python",
   "name": "rag_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
