{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Retrieval Baseline for Omnilex Legal Retrieval\n",
    "\n",
    "This notebook implements an **agentic retrieval approach** using a ReAct-style agent with search tools.\n",
    "\n",
    "## Approach\n",
    "1. Load a local LLM (GGUF format via llama-cpp-python)\n",
    "2. Build BM25 search indices for laws and court decisions\n",
    "3. Create search tools the agent can use\n",
    "4. For each query, run a ReAct agent that:\n",
    "   - Reasons about what to search\n",
    "   - Uses tools to search laws and court decisions\n",
    "   - Extracts citations from search results\n",
    "   - Provides final answer with all found citations\n",
    "\n",
    "## Advantages over Direct Generation\n",
    "- Grounded in actual legal documents\n",
    "- Less hallucination of non-existent citations\n",
    "- Can iterate on searches to find more relevant sources\n",
    "\n",
    "## Requirements\n",
    "- llama-cpp-python\n",
    "- rank-bm25\n",
    "- A GGUF model file (e.g., Mistral-7B-Instruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nfrom pathlib import Path\n\n# === CONFIGURATION ===\n# Choose which dataset to run on: \"val\" or \"test\"\nDATASET_MODE = \"val\"  # Change to \"test\" for final submission\n\n# Set to True to rebuild indices from CSV (required on first run)\n# Set to False to load cached indices (faster for subsequent runs)\nFORCE_REBUILD_INDICES = False\n\n# Detect environment\nKAGGLE_ENV = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n\nif KAGGLE_ENV:\n    # Kaggle paths\n    DATA_PATH = Path(\"/kaggle/input/omnilex-data\")\n    MODEL_PATH = Path(\"/kaggle/input/llama-model\")\n    OUTPUT_PATH = Path(\"/kaggle/working\")\n    INDEX_PATH = Path(\"/kaggle/input/omnilex-indices\")\n    sys.path.insert(0, \"/kaggle/input/omnilex-utils\")\nelse:\n    # Local development paths\n    REPO_ROOT = Path(\".\").resolve().parent\n    DATA_PATH = REPO_ROOT / \"data\"\n    MODEL_PATH = REPO_ROOT / \"models\"\n    OUTPUT_PATH = REPO_ROOT / \"output\"\n    INDEX_PATH = REPO_ROOT / \"data\" / \"processed\"\n    sys.path.insert(0, str(REPO_ROOT / \"src\"))\n\n# CSV corpus files for index building\nLAWS_CSV = DATA_PATH / \"laws_de.csv\"\nCOURTS_CSV = DATA_PATH / \"court_considerations.csv\"\n\n# Index cache paths\nLAWS_INDEX_PATH = INDEX_PATH / \"laws_index.pkl\"\nCOURTS_INDEX_PATH = INDEX_PATH / \"courts_index.pkl\"\n\n# Derived paths based on DATASET_MODE\nQUERY_FILE = DATA_PATH / f\"{DATASET_MODE}.csv\"\nIS_VALIDATION_MODE = DATASET_MODE == \"val\"\n\n# Create output directory\nOUTPUT_PATH.mkdir(parents=True, exist_ok=True)\nINDEX_PATH.mkdir(parents=True, exist_ok=True)\n\nprint(f\"Environment: {'Kaggle' if KAGGLE_ENV else 'Local'}\")\nprint(f\"Dataset mode: {DATASET_MODE}\")\nprint(f\"Query file: {QUERY_FILE}\")\nprint(f\"Validation mode: {IS_VALIDATION_MODE}\")\nprint(f\"Force rebuild indices: {FORCE_REBUILD_INDICES}\")\nprint(f\"\\nCorpus files:\")\nprint(f\"  Laws CSV: {LAWS_CSV} ({LAWS_CSV.stat().st_size / 1e6:.1f} MB)\" if LAWS_CSV.exists() else f\"  Laws CSV: {LAWS_CSV} (NOT FOUND)\")\nprint(f\"  Courts CSV: {COURTS_CSV} ({COURTS_CSV.stat().st_size / 1e9:.2f} GB)\" if COURTS_CSV.exists() else f\"  Courts CSV: {COURTS_CSV} (NOT FOUND)\")\nprint(f\"\\nIndex cache: {INDEX_PATH}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Model settings\n",
    "    \"model_file\": \"mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n",
    "    \"n_ctx\": 8192,         # Larger context for agent conversations\n",
    "    \"n_threads\": 4,\n",
    "    \"n_gpu_layers\": -1,    # GPU layers (-1 = offload all layers to GPU)\n",
    "    \n",
    "    # Agent settings\n",
    "    \"max_iterations\": 5,   # Max agent iterations per query\n",
    "    \"max_tokens\": 512,\n",
    "    \"temperature\": 0.1,\n",
    "    \n",
    "    # Retrieval settings\n",
    "    \"top_k_laws\": 5,       # Results per law search\n",
    "    \"top_k_courts\": 5,     # Results per court search\n",
    "    \n",
    "    # Paths\n",
    "    \"test_file\": \"test.csv\",\n",
    "    \"train_file\": \"train.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Corpora and Build/Load Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nfrom tqdm.notebook import tqdm\nfrom omnilex.retrieval.bm25_index import BM25Index\n\n\ndef load_csv_corpus(\n    csv_path: Path,\n    chunk_size: int = 100_000,\n    max_rows: int | None = None\n) -> list[dict]:\n    \"\"\"Load CSV corpus into list of dicts with progress bar.\n    \n    Args:\n        csv_path: Path to CSV file with 'citation' and 'text' columns\n        chunk_size: Rows to process per chunk (for memory efficiency)\n        max_rows: Optional limit on rows (for testing with smaller corpus)\n    \n    Returns:\n        List of {\"citation\": str, \"text\": str} dicts\n    \"\"\"\n    documents = []\n    \n    # Count rows for progress bar (fast line count)\n    print(f\"Counting rows in {csv_path.name}...\")\n    with open(csv_path, encoding='utf-8') as f:\n        total_rows = sum(1 for _ in f) - 1  # minus header\n    \n    if max_rows:\n        total_rows = min(total_rows, max_rows)\n    print(f\"Total rows to load: {total_rows:,}\")\n    \n    rows_loaded = 0\n    with tqdm(total=total_rows, desc=f\"Loading {csv_path.name}\") as pbar:\n        for chunk in pd.read_csv(csv_path, chunksize=chunk_size):\n            for _, row in chunk.iterrows():\n                if max_rows and rows_loaded >= max_rows:\n                    break\n                documents.append({\n                    \"citation\": str(row[\"citation\"]),\n                    \"text\": str(row[\"text\"]) if pd.notna(row[\"text\"]) else \"\"\n                })\n                rows_loaded += 1\n            pbar.update(min(len(chunk), total_rows - pbar.n))\n            if max_rows and rows_loaded >= max_rows:\n                break\n    \n    return documents\n\n\ndef get_or_build_index(\n    name: str,\n    csv_path: Path,\n    index_path: Path,\n    force_rebuild: bool = False,\n    max_rows: int | None = None\n) -> BM25Index:\n    \"\"\"Load cached index or build from CSV.\n    \n    Args:\n        name: Index name for logging\n        csv_path: Path to corpus CSV\n        index_path: Path to cache index pickle\n        force_rebuild: If True, rebuild even if cache exists\n        max_rows: Optional row limit (for testing with smaller corpus)\n    \n    Returns:\n        BM25Index instance\n    \"\"\"\n    # Use cached index if available and not forcing rebuild\n    if index_path.exists() and not force_rebuild:\n        print(f\"Loading cached {name} index from {index_path}\")\n        index = BM25Index.load(index_path)\n        print(f\"  Loaded {len(index.documents):,} documents\")\n        return index\n    \n    # Check CSV exists\n    if not csv_path.exists():\n        print(f\"Warning: {csv_path} not found. Creating empty index.\")\n        return BM25Index(documents=[])\n    \n    # Load corpus from CSV\n    print(f\"\\n{'='*50}\")\n    print(f\"Building {name} index from {csv_path}\")\n    print(f\"{'='*50}\")\n    documents = load_csv_corpus(csv_path, max_rows=max_rows)\n    \n    if not documents:\n        print(f\"Warning: No documents loaded. Creating empty index.\")\n        return BM25Index(documents=[])\n    \n    # Build BM25 index\n    print(f\"\\nBuilding BM25 index for {len(documents):,} documents...\")\n    index = BM25Index(\n        documents=documents,\n        text_field=\"text\",\n        citation_field=\"citation\"\n    )\n    print(f\"Index built successfully!\")\n    \n    # Cache index for future runs\n    if not KAGGLE_ENV:\n        print(f\"Saving index to {index_path}...\")\n        index.save(index_path)\n        print(f\"Index cached.\")\n    \n    return index"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load or build laws index\n# Laws CSV: ~45MB, ~269K rows\n# Build time: ~30 seconds | Load from cache: <1 second\n\nlaws_index = get_or_build_index(\n    name=\"laws\",\n    csv_path=LAWS_CSV,\n    index_path=LAWS_INDEX_PATH,\n    force_rebuild=FORCE_REBUILD_INDICES,\n    # max_rows=10000  # Uncomment to test with smaller corpus\n)\nprint(f\"\\nLaws index: {len(laws_index.documents):,} documents\")\n\n# Test search\ntest_results = laws_index.search(\"Vertrag\", top_k=3)\nprint(f\"\\nTest search 'Vertrag': {len(test_results)} results\")\nif test_results:\n    print(f\"  Top result: {test_results[0].get('citation', 'N/A')}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load or build courts index\n# Courts CSV: ~2.3GB, ~2.5M rows\n# Build time: ~15-20 minutes | Load from cache: ~10 seconds\n# Peak memory during build: ~8-10GB\n\ncourts_index = get_or_build_index(\n    name=\"courts\",\n    csv_path=COURTS_CSV,\n    index_path=COURTS_INDEX_PATH,\n    force_rebuild=FORCE_REBUILD_INDICES,\n    # max_rows=100000  # Uncomment to test with smaller corpus\n)\nprint(f\"\\nCourts index: {len(courts_index.documents):,} documents\")\n\n# Test search\ntest_results = courts_index.search(\"Meinungsfreiheit\", top_k=3)\nprint(f\"\\nTest search 'Meinungsfreiheit': {len(test_results)} results\")\nif test_results:\n    print(f\"  Top result: {test_results[0].get('citation', 'N/A')}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Search Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools registered:\n",
      "  - search_laws: Search Swiss federal laws (SR/Systematische Rechtssammlung) by keywords.\n",
      "  - search_courts: Search Swiss Federal Court decisions (BGE) by keywords.\n"
     ]
    }
   ],
   "source": [
    "from omnilex.retrieval.tools import LawSearchTool, CourtSearchTool\n",
    "\n",
    "# Create tools\n",
    "law_tool = LawSearchTool(\n",
    "    index=laws_index,\n",
    "    top_k=CONFIG[\"top_k_laws\"],\n",
    "    max_excerpt_length=300,\n",
    ")\n",
    "\n",
    "court_tool = CourtSearchTool(\n",
    "    index=courts_index,\n",
    "    top_k=CONFIG[\"top_k_courts\"],\n",
    "    max_excerpt_length=300,\n",
    ")\n",
    "\n",
    "# Tool registry\n",
    "TOOLS = {\n",
    "    \"search_laws\": law_tool,\n",
    "    \"search_courts\": court_tool,\n",
    "}\n",
    "\n",
    "print(\"Tools registered:\")\n",
    "for name, tool in TOOLS.items():\n",
    "    print(f\"  - {name}: {tool.description.split(chr(10))[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing law search:\n",
      "No relevant federal laws found for: 'Vertrag Abschluss'\n",
      "\n",
      "Testing court search:\n",
      "No relevant court decisions found for: 'Meinungsfreiheit'\n"
     ]
    }
   ],
   "source": [
    "# Test tools\n",
    "print(\"Testing law search:\")\n",
    "print(law_tool(\"Vertrag Abschluss\"))\n",
    "\n",
    "print(\"\\nTesting court search:\")\n",
    "print(court_tool(\"Meinungsfreiheit\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: /home/arijo/Omnilex-Agentic-Retrieval-Competition/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Running on: GPU (all layers offloaded)\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "from omnilex.llm import has_cuda_support, get_device_info\n",
    "\n",
    "# Find model file\n",
    "model_file = MODEL_PATH / CONFIG[\"model_file\"]\n",
    "\n",
    "if not model_file.exists():\n",
    "    gguf_files = list(MODEL_PATH.glob(\"*.gguf\")) + list(MODEL_PATH.rglob(\"*.gguf\"))\n",
    "    if gguf_files:\n",
    "        model_file = gguf_files[0]\n",
    "        print(f\"Using model: {model_file}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No model found. Please download a GGUF model to {MODEL_PATH}\"\n",
    "        )\n",
    "\n",
    "print(f\"Loading model: {model_file}\")\n",
    "\n",
    "# Auto-detect GPU: use GPU if available, else CPU\n",
    "n_gpu_layers = CONFIG[\"n_gpu_layers\"]\n",
    "if n_gpu_layers == -1 and not has_cuda_support():\n",
    "    n_gpu_layers = 0  # Fallback to CPU if no CUDA support\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=str(model_file),\n",
    "    n_ctx=CONFIG[\"n_ctx\"],\n",
    "    n_threads=CONFIG[\"n_threads\"],\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Running on: {get_device_info(n_gpu_layers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "AGENT_SYSTEM_PROMPT = \"\"\"You are a Swiss legal research assistant with access to two search tools:\n",
    "\n",
    "1. search_laws(query): Search Swiss federal laws (SR/Systematische Rechtssammlung) by keywords\n",
    "   - Returns relevant law provisions with citations and text excerpts\n",
    "   - Use for finding statutory law: codes, acts, ordinances\n",
    "\n",
    "2. search_courts(query): Search Swiss Federal Court decisions (BGE/Bundesgerichtsentscheide) by keywords\n",
    "   - Returns relevant case law with citations and excerpts\n",
    "   - Use for finding judicial interpretations and precedents\n",
    "\n",
    "Your task is to find ALL relevant Swiss legal citations for the given query.\n",
    "\n",
    "Instructions:\n",
    "- Search BOTH laws AND court decisions for comprehensive results\n",
    "- Use multiple search queries if needed (different terms, German/English)\n",
    "- Extract citations in standard format: SR XXX Art. Y or BGE XXX YY ZZZ\n",
    "- Continue searching until you have found all relevant sources\n",
    "\n",
    "Format your response as:\n",
    "Thought: [Your reasoning about what to search next]\n",
    "Action: [tool_name]\n",
    "Action Input: [search query]\n",
    "\n",
    "After receiving results, either continue searching or provide final answer:\n",
    "Final Answer: [List of all found citations, one per line]\n",
    "\n",
    "Remember: Always search both laws AND court decisions before giving your final answer.\"\"\"\n",
    "\n",
    "\n",
    "def parse_agent_action(response: str):\n",
    "    \"\"\"Parse action and input from agent response.\"\"\"\n",
    "    action_match = re.search(r\"Action:\\s*(\\w+)\", response, re.IGNORECASE)\n",
    "    input_match = re.search(r\"Action Input:\\s*(.+?)(?:\\n|$)\", response, re.IGNORECASE)\n",
    "    \n",
    "    if action_match and input_match:\n",
    "        return action_match.group(1).strip(), input_match.group(1).strip()\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def extract_citations_from_text(text: str) -> list[str]:\n",
    "    \"\"\"Extract citations from any text (tool output or final answer).\"\"\"\n",
    "    citations = []\n",
    "    \n",
    "    # SR pattern: SR followed by number (optionally with article)\n",
    "    sr_matches = re.findall(\n",
    "        r\"SR\\s*\\d{3}(?:\\.\\d+)?(?:\\s+Art\\.?\\s*\\d+[a-z]?)?\",\n",
    "        text,\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    citations.extend(sr_matches)\n",
    "    \n",
    "    # BGE pattern: BGE volume section page\n",
    "    bge_matches = re.findall(\n",
    "        r\"BGE\\s+\\d{1,3}\\s+[IVX]+[a-z]?\\s+\\d+(?:\\s+E\\.\\s*\\d+[a-z]?)?\",\n",
    "        text,\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    citations.extend(bge_matches)\n",
    "    \n",
    "    # Art. pattern: Art. X LAW (e.g., Art. 1 ZGB, Art. 41 OR)\n",
    "    art_matches = re.findall(\n",
    "        r\"Art\\.?\\s+\\d+[a-z]?\\s+(?:Abs\\.?\\s*\\d+\\s+)?[A-Z]{2,}\",\n",
    "        text,\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    citations.extend(art_matches)\n",
    "    \n",
    "    return list(set(citations))\n",
    "\n",
    "\n",
    "def run_agent(query: str, verbose: bool = False) -> list[str]:\n",
    "    \"\"\"Run ReAct agent to retrieve citations.\"\"\"\n",
    "    # Format with Mistral Instruct tags\n",
    "    conversation = f\"[INST] {AGENT_SYSTEM_PROMPT}\\n\\nQuery: {query}\\n\\nThought: [/INST]\"\n",
    "    all_citations = []\n",
    "    \n",
    "    for iteration in range(CONFIG[\"max_iterations\"]):\n",
    "        # Get LLM response\n",
    "        response = llm(\n",
    "            conversation,\n",
    "            max_tokens=CONFIG[\"max_tokens\"],\n",
    "            temperature=CONFIG[\"temperature\"],\n",
    "            stop=[\"Observation:\", \"[INST]\", \"</s>\"],\n",
    "        )[\"choices\"][0][\"text\"]\n",
    "        \n",
    "        # For subsequent turns, we need to handle the conversation format\n",
    "        if iteration == 0:\n",
    "            conversation = f\"[INST] {AGENT_SYSTEM_PROMPT}\\n\\nQuery: {query} [/INST]\\n\\nThought:{response}\"\n",
    "        else:\n",
    "            conversation += response\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n[Iteration {iteration + 1}]\")\n",
    "            print(response[:500])\n",
    "        \n",
    "        # Check for final answer\n",
    "        if \"Final Answer:\" in response:\n",
    "            final_text = response.split(\"Final Answer:\")[-1].strip()\n",
    "            citations = extract_citations_from_text(final_text)\n",
    "            all_citations.extend(citations)\n",
    "            break\n",
    "        \n",
    "        # Parse and execute action\n",
    "        action, action_input = parse_agent_action(response)\n",
    "        \n",
    "        if action and action_input:\n",
    "            action_lower = action.lower()\n",
    "            \n",
    "            if action_lower in TOOLS:\n",
    "                observation = TOOLS[action_lower](action_input)\n",
    "                \n",
    "                # Extract citations from observation\n",
    "                obs_citations = extract_citations_from_text(observation)\n",
    "                all_citations.extend(obs_citations)\n",
    "                \n",
    "                conversation += f\"\\nObservation: {observation}\\n\\n[INST] Continue your analysis. [/INST]\\n\\nThought:\"\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"\\n[Tool: {action}]\")\n",
    "                    print(observation[:300])\n",
    "            else:\n",
    "                conversation += f\"\\nObservation: Unknown tool '{action}'. Available: search_laws, search_courts\\n\\n[INST] Continue. [/INST]\\n\\nThought:\"\n",
    "        else:\n",
    "            # No action found, try to extract from response anyway\n",
    "            citations = extract_citations_from_text(response)\n",
    "            all_citations.extend(citations)\n",
    "            break\n",
    "    \n",
    "    # Deduplicate\n",
    "    return list(set(all_citations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the requirements for a valid contract under Swiss law?\n",
      "\n",
      "Running agent...\n",
      "\n",
      "\n",
      "[Iteration 1]\n",
      " To find the requirements for a valid contract under Swiss law, I will first search for relevant provisions in the Swiss federal laws using the term \"Vertrag\" which is the German word for contract.\n",
      "\n",
      "Action: search_laws\n",
      "Action Input: Vertrag\n",
      "\n",
      "[Waiting for results]\n",
      "\n",
      "Thought: The search results from the laws might not be comprehensive, so I will also look for relevant court decisions interpreting the requirements for a valid contract in Swiss law.\n",
      "\n",
      "Action: search_courts\n",
      "Action Input: Valid contract\n",
      "\n",
      "==================================================\n",
      "Found citations:\n",
      "  - Art. 1\n",
      "BGE\n",
      "  - SR 111.2 Art. 1\n",
      "  - BGE 135 I 123\n",
      "  - BGE 123 IV 567\n",
      "  - Art. 31\n",
      "SR\n",
      "  - SR 111.1 Art. 31\n"
     ]
    }
   ],
   "source": [
    "# Test agent with a sample query\n",
    "test_query = \"What are the requirements for a valid contract under Swiss law?\"\n",
    "print(f\"Query: {test_query}\")\n",
    "print(\"\\nRunning agent...\\n\")\n",
    "\n",
    "citations = run_agent(test_query, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Found citations:\")\n",
    "for c in citations:\n",
    "    print(f\"  - {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\n\n# Load queries from the configured query file\nif not QUERY_FILE.exists():\n    raise FileNotFoundError(f\"Query file not found: {QUERY_FILE}\")\n\ntest_df = pd.read_csv(QUERY_FILE)\n\nprint(f\"Loaded {len(test_df)} queries from {QUERY_FILE}\")\nprint(f\"Columns: {list(test_df.columns)}\")\n\nif IS_VALIDATION_MODE and \"gold_citations\" in test_df.columns:\n    print(f\"Gold citations available for evaluation\")\n\ntest_df.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running agent:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running agent: 100%|██████████| 2/2 [00:27<00:00, 13.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated predictions for 2 queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from omnilex.citations.normalizer import CitationNormalizer\n",
    "\n",
    "# Initialize normalizer\n",
    "normalizer = CitationNormalizer()\n",
    "\n",
    "# Generate predictions\n",
    "predictions = []\n",
    "\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Running agent\"):\n",
    "    query_id = row[\"query_id\"]\n",
    "    query_text = row[\"query\"]\n",
    "    \n",
    "    # Run agent\n",
    "    raw_citations = run_agent(query_text, verbose=False)\n",
    "    \n",
    "    # Normalize citations\n",
    "    normalized = normalizer.canonicalize_list(raw_citations)\n",
    "    \n",
    "    predictions.append({\n",
    "        \"query_id\": query_id,\n",
    "        \"predicted_citations\": \";\".join(normalized),\n",
    "    })\n",
    "\n",
    "print(f\"\\nGenerated predictions for {len(predictions)} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>predicted_citations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_001</td>\n",
       "      <td>BGE 143 II 168;BGE 161 II 121;BGE 154 II 135;B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_002</td>\n",
       "      <td>BGE 113 I 112;BGE 111 II 158;BGE 125 II 167;BG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id                                predicted_citations\n",
       "0  test_001  BGE 143 II 168;BGE 161 II 121;BGE 154 II 135;B...\n",
       "1  test_002  BGE 113 I 112;BGE 111 II 158;BGE 125 II 167;BG..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview predictions\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to: /home/arijo/Omnilex-Agentic-Retrieval-Competition/output/submission.csv\n",
      "Total predictions: 2\n",
      "\n",
      "Sample submission:\n",
      "   query_id                                predicted_citations\n",
      "0  test_001  BGE 143 II 168;BGE 161 II 121;BGE 154 II 135;B...\n",
      "1  test_002  BGE 113 I 112;BGE 111 II 158;BGE 125 II 167;BG...\n"
     ]
    }
   ],
   "source": [
    "# Save submission\n",
    "submission_path = OUTPUT_PATH / \"submission.csv\"\n",
    "predictions_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Submission saved to: {submission_path}\")\n",
    "print(f\"Total predictions: {len(predictions_df)}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample submission:\")\n",
    "print(predictions_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Local Evaluation (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate if in validation mode with gold labels\nif IS_VALIDATION_MODE and \"gold_citations\" in test_df.columns:\n    from omnilex.evaluation import evaluate_submission\n    \n    # Join predictions with gold citations from the same file\n    eval_df = predictions_df.merge(\n        test_df[[\"query_id\", \"gold_citations\"]],\n        on=\"query_id\",\n        how=\"inner\"\n    )\n    \n    if len(eval_df) > 0:\n        scores = evaluate_submission(\n            eval_df[[\"query_id\", \"predicted_citations\"]],\n            eval_df[[\"query_id\", \"gold_citations\"]],\n        )\n        \n        print(\"\\n\" + \"=\"*50)\n        print(\"EVALUATION RESULTS\")\n        print(\"=\"*50)\n        print(f\"Queries evaluated: {len(eval_df)}\")\n        print(f\"\\nMacro F1 (PRIMARY): {scores['macro_f1']:.4f}\")\n        print(f\"Macro Precision:    {scores['macro_precision']:.4f}\")\n        print(f\"Macro Recall:       {scores['macro_recall']:.4f}\")\n        print(f\"\\nMAP:                {scores['map']:.4f}\")\n    else:\n        print(\"No overlapping queries for evaluation.\")\nelse:\n    print(\"Skipping evaluation (not in validation mode or no gold labels available)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This agentic retrieval baseline demonstrates a more sophisticated approach:\n",
    "\n",
    "1. **Tool-augmented generation**: The LLM can search actual legal corpora rather than relying solely on parametric knowledge.\n",
    "\n",
    "2. **ReAct-style reasoning**: The agent reasons about what to search, executes searches, observes results, and iterates.\n",
    "\n",
    "3. **Grounded citations**: Citations are extracted from actual search results, reducing hallucination.\n",
    "\n",
    "4. **Comprehensive search**: The agent searches both laws and court decisions for complete results.\n",
    "\n",
    "## Potential Improvements\n",
    "\n",
    "- **Better search**: Use semantic search (embeddings) instead of BM25\n",
    "- **Query expansion**: Generate multiple search queries in different languages\n",
    "- **Relevance filtering**: Add a step to verify citations are actually relevant\n",
    "- **Citation validation**: Check that generated citations exist in the corpus\n",
    "- **Multi-hop reasoning**: Follow citation chains to find related sources"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}