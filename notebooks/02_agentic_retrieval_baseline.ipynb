{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Retrieval Baseline for Omnilex Legal Retrieval\n",
    "\n",
    "This notebook implements an **agentic retrieval approach** using a ReAct-style agent with search tools.\n",
    "\n",
    "## Approach\n",
    "1. Load a local LLM (GGUF format via llama-cpp-python)\n",
    "2. Build BM25 search indices for laws and court decisions\n",
    "3. Create search tools the agent can use\n",
    "4. For each query, run a ReAct agent that:\n",
    "   - Reasons about what to search\n",
    "   - Uses tools to search laws and court decisions\n",
    "   - Extracts citations from search results\n",
    "   - Provides final answer with all found citations\n",
    "\n",
    "## Advantages over Direct Generation\n",
    "- Grounded in actual legal documents\n",
    "- Less hallucination of non-existent citations\n",
    "- Can iterate on searches to find more relevant sources\n",
    "\n",
    "## Requirements\n",
    "- llama-cpp-python\n",
    "- rank-bm25\n",
    "- A GGUF model file (e.g., Mistral-7B-Instruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "# Choose which dataset to run on: \"val\" or \"test\"\n",
    "DATASET_MODE = \"val\"  # Change to \"test\" for final submission\n",
    "\n",
    "# Set to True to rebuild indices from CSV (required on first run)\n",
    "# Set to False to load cached indices (faster for subsequent runs)\n",
    "FORCE_REBUILD_INDICES = False\n",
    "\n",
    "# Detect environment\n",
    "KAGGLE_ENV = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "\n",
    "if KAGGLE_ENV:\n",
    "    # Kaggle paths\n",
    "    DATA_PATH = Path(\"/kaggle/input/omnilex-data\")\n",
    "    MODEL_PATH = Path(\"/kaggle/input/llama-model\")\n",
    "    OUTPUT_PATH = Path(\"/kaggle/working\")\n",
    "    INDEX_PATH = Path(\"/kaggle/input/omnilex-indices\")\n",
    "    sys.path.insert(0, \"/kaggle/input/omnilex-utils\")\n",
    "else:\n",
    "    # Local development paths\n",
    "    REPO_ROOT = Path(\".\").resolve().parent\n",
    "    DATA_PATH = REPO_ROOT / \"data\"\n",
    "    MODEL_PATH = REPO_ROOT / \"models\"\n",
    "    OUTPUT_PATH = REPO_ROOT / \"output\"\n",
    "    INDEX_PATH = REPO_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "# CSV corpus files for index building\n",
    "LAWS_CSV = DATA_PATH / \"laws_de.csv\"\n",
    "COURTS_CSV = DATA_PATH / \"court_considerations.csv\"\n",
    "\n",
    "# Index cache paths\n",
    "LAWS_INDEX_PATH = INDEX_PATH / \"laws_index.pkl\"\n",
    "COURTS_INDEX_PATH = INDEX_PATH / \"courts_index.pkl\"\n",
    "\n",
    "# Derived paths based on DATASET_MODE\n",
    "QUERY_FILE = DATA_PATH / f\"{DATASET_MODE}.csv\"\n",
    "IS_VALIDATION_MODE = DATASET_MODE == \"val\"\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "INDEX_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Environment: {'Kaggle' if KAGGLE_ENV else 'Local'}\")\n",
    "print(f\"Dataset mode: {DATASET_MODE}\")\n",
    "print(f\"Query file: {QUERY_FILE}\")\n",
    "print(f\"Validation mode: {IS_VALIDATION_MODE}\")\n",
    "print(f\"Force rebuild indices: {FORCE_REBUILD_INDICES}\")\n",
    "print(f\"\\nCorpus files:\")\n",
    "print(f\"  Laws CSV: {LAWS_CSV} ({LAWS_CSV.stat().st_size / 1e6:.1f} MB)\" if LAWS_CSV.exists() else f\"  Laws CSV: {LAWS_CSV} (NOT FOUND)\")\n",
    "print(f\"  Courts CSV: {COURTS_CSV} ({COURTS_CSV.stat().st_size / 1e9:.2f} GB)\" if COURTS_CSV.exists() else f\"  Courts CSV: {COURTS_CSV} (NOT FOUND)\")\n",
    "print(f\"\\nIndex cache: {INDEX_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Model settings\n",
    "    \"model_file\": \"mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n",
    "    \"n_ctx\": 8192,         # Context window size\n",
    "    \"n_threads\": 4,\n",
    "    \"n_gpu_layers\": -1,    # GPU layers (-1 = offload all layers to GPU)\n",
    "    \n",
    "    # Agent settings\n",
    "    \"max_iterations\": 3,   # Max agent iterations per query\n",
    "    \"max_tokens\": 512,\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_observation_chars\": 1200,  # Reduced from 2000 to prevent context overflow\n",
    "    \"max_conversation_chars\": 28000,  # Safety net: truncate if conversation exceeds this\n",
    "    \n",
    "    # Retrieval settings\n",
    "    \"top_k_laws\": 40,       # Results per law search\n",
    "    \"top_k_courts\": 40,     # Results per court search\n",
    "    \n",
    "    # Paths\n",
    "    \"test_file\": \"test.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Corpora and Build/Load Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import re\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "\n",
    "class BM25Index:\n",
    "    \"\"\"BM25 index for keyword search over legal documents.\n",
    "\n",
    "    Supports Swiss federal laws (SR) and court decisions (BGE).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        documents: list[dict] | None = None,\n",
    "        text_field: str = \"text\",\n",
    "        citation_field: str = \"citation\",\n",
    "    ):\n",
    "        \"\"\"Initialize BM25 index.\n",
    "\n",
    "        Args:\n",
    "            documents: List of document dictionaries\n",
    "            text_field: Key for document text in dict\n",
    "            citation_field: Key for citation string in dict\n",
    "        \"\"\"\n",
    "        self.text_field = text_field\n",
    "        self.citation_field = citation_field\n",
    "\n",
    "        self.documents: list[dict] = []\n",
    "        self.index: BM25Okapi | None = None\n",
    "        self._tokenized_corpus: list[list[str]] = []\n",
    "\n",
    "        if documents:\n",
    "            self.build(documents)\n",
    "\n",
    "    def tokenize(self, text: str) -> list[str]:\n",
    "        \"\"\"Tokenize text for BM25 indexing.\n",
    "\n",
    "        Simple whitespace + lowercase tokenization.\n",
    "        Can be overridden for language-specific tokenization.\n",
    "\n",
    "        Args:\n",
    "            text: Text to tokenize\n",
    "\n",
    "        Returns:\n",
    "            List of tokens\n",
    "        \"\"\"\n",
    "        # Lowercase and split on non-alphanumeric characters\n",
    "        text = text.lower()\n",
    "        tokens = re.split(r\"\\W+\", text)\n",
    "        # Filter empty tokens\n",
    "        return [t for t in tokens if t]\n",
    "\n",
    "    def build(self, documents: list[dict]) -> None:\n",
    "        \"\"\"Build BM25 index from documents.\n",
    "\n",
    "        Args:\n",
    "            documents: List of document dictionaries\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "\n",
    "        # Tokenize all documents\n",
    "        self._tokenized_corpus = []\n",
    "        for doc in documents:\n",
    "            text = doc.get(self.text_field, \"\")\n",
    "            tokens = self.tokenize(text)\n",
    "            self._tokenized_corpus.append(tokens)\n",
    "\n",
    "        # Build BM25 index\n",
    "        self.index = BM25Okapi(self._tokenized_corpus)\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 10,\n",
    "        return_scores: bool = False,\n",
    "    ) -> list[dict]:\n",
    "        \"\"\"Search the index with a query.\n",
    "\n",
    "        Args:\n",
    "            query: Search query string\n",
    "            top_k: Number of results to return\n",
    "            return_scores: Whether to include BM25 scores in results\n",
    "\n",
    "        Returns:\n",
    "            List of matching documents (with optional scores)\n",
    "        \"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not built. Call build() first.\")\n",
    "\n",
    "        # Tokenize query\n",
    "        query_tokens = self.tokenize(query)\n",
    "\n",
    "        if not query_tokens:\n",
    "            return []\n",
    "\n",
    "        # Get BM25 scores\n",
    "        scores = self.index.get_scores(query_tokens)\n",
    "\n",
    "        # Get top-k indices\n",
    "        top_indices = scores.argsort()[-top_k:][::-1]\n",
    "\n",
    "        # Build results\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            if scores[idx] <= 0:\n",
    "                continue\n",
    "\n",
    "            doc = self.documents[idx].copy()\n",
    "            if return_scores:\n",
    "                doc[\"_score\"] = float(scores[idx])\n",
    "            results.append(doc)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def save(self, path: Path | str) -> None:\n",
    "        \"\"\"Save index to disk.\n",
    "\n",
    "        Args:\n",
    "            path: Path to save index (creates .pkl file)\n",
    "        \"\"\"\n",
    "        path = Path(path)\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        data = {\n",
    "            \"documents\": self.documents,\n",
    "            \"tokenized_corpus\": self._tokenized_corpus,\n",
    "            \"text_field\": self.text_field,\n",
    "            \"citation_field\": self.citation_field,\n",
    "        }\n",
    "\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: Path | str) -> \"BM25Index\":\n",
    "        \"\"\"Load index from disk.\n",
    "\n",
    "        Args:\n",
    "            path: Path to saved index\n",
    "\n",
    "        Returns:\n",
    "            Loaded BM25Index instance\n",
    "        \"\"\"\n",
    "        path = Path(path)\n",
    "\n",
    "        with open(path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        instance = cls(\n",
    "            text_field=data[\"text_field\"],\n",
    "            citation_field=data.get(\"citation_field\", \"citation\"),\n",
    "        )\n",
    "        instance.documents = data[\"documents\"]\n",
    "        instance._tokenized_corpus = data[\"tokenized_corpus\"]\n",
    "        instance.index = BM25Okapi(instance._tokenized_corpus)\n",
    "\n",
    "        return instance\n",
    "\n",
    "\n",
    "def load_csv_corpus(\n",
    "    csv_path: Path,\n",
    "    chunk_size: int = 100_000,\n",
    "    max_rows: int | None = None\n",
    ") -> list[dict]:\n",
    "    \"\"\"Load CSV corpus into list of dicts with progress bar.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to CSV file with 'citation' and 'text' columns\n",
    "        chunk_size: Rows to process per chunk (for memory efficiency)\n",
    "        max_rows: Optional limit on rows (for testing with smaller corpus)\n",
    "    \n",
    "    Returns:\n",
    "        List of {\"citation\": str, \"text\": str} dicts\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    # Count rows for progress bar (fast line count)\n",
    "    print(f\"Counting rows in {csv_path.name}...\")\n",
    "    with open(csv_path, encoding='utf-8') as f:\n",
    "        total_rows = sum(1 for _ in f) - 1  # minus header\n",
    "    \n",
    "    if max_rows:\n",
    "        total_rows = min(total_rows, max_rows)\n",
    "    print(f\"Total rows to load: {total_rows:,}\")\n",
    "    \n",
    "    rows_loaded = 0\n",
    "    with tqdm(total=total_rows, desc=f\"Loading {csv_path.name}\") as pbar:\n",
    "        for chunk in pd.read_csv(csv_path, chunksize=chunk_size):\n",
    "            for _, row in chunk.iterrows():\n",
    "                if max_rows and rows_loaded >= max_rows:\n",
    "                    break\n",
    "                documents.append({\n",
    "                    \"citation\": str(row[\"citation\"]),\n",
    "                    \"text\": str(row[\"text\"]) if pd.notna(row[\"text\"]) else \"\"\n",
    "                })\n",
    "                rows_loaded += 1\n",
    "            pbar.update(min(len(chunk), total_rows - pbar.n))\n",
    "            if max_rows and rows_loaded >= max_rows:\n",
    "                break\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "def get_or_build_index(\n",
    "    name: str,\n",
    "    csv_path: Path,\n",
    "    index_path: Path,\n",
    "    force_rebuild: bool = False,\n",
    "    max_rows: int | None = None\n",
    ") -> BM25Index:\n",
    "    \"\"\"Load cached index or build from CSV.\n",
    "    \n",
    "    Args:\n",
    "        name: Index name for logging\n",
    "        csv_path: Path to corpus CSV\n",
    "        index_path: Path to cache index pickle\n",
    "        force_rebuild: If True, rebuild even if cache exists\n",
    "        max_rows: Optional row limit (for testing with smaller corpus)\n",
    "    \n",
    "    Returns:\n",
    "        BM25Index instance\n",
    "    \"\"\"\n",
    "    # Use cached index if available and not forcing rebuild\n",
    "    if index_path.exists() and not force_rebuild:\n",
    "        print(f\"Loading cached {name} index from {index_path}\")\n",
    "        index = BM25Index.load(index_path)\n",
    "        print(f\"  Loaded {len(index.documents):,} documents\")\n",
    "        return index\n",
    "    \n",
    "    # Check CSV exists\n",
    "    if not csv_path.exists():\n",
    "        print(f\"Warning: {csv_path} not found. Creating empty index.\")\n",
    "        return BM25Index(documents=[])\n",
    "    \n",
    "    # Load corpus from CSV\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Building {name} index from {csv_path}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    documents = load_csv_corpus(csv_path, max_rows=max_rows)\n",
    "    \n",
    "    if not documents:\n",
    "        print(f\"Warning: No documents loaded. Creating empty index.\")\n",
    "        return BM25Index(documents=[])\n",
    "    \n",
    "    # Build BM25 index\n",
    "    print(f\"\\nBuilding BM25 index for {len(documents):,} documents...\")\n",
    "    index = BM25Index(\n",
    "        documents=documents,\n",
    "        text_field=\"text\",\n",
    "        citation_field=\"citation\"\n",
    "    )\n",
    "    print(f\"Index built successfully!\")\n",
    "    \n",
    "    # Cache index for future runs\n",
    "    if not KAGGLE_ENV:\n",
    "        print(f\"Saving index to {index_path}...\")\n",
    "        index.save(index_path)\n",
    "        print(f\"Index cached.\")\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or build laws index\n",
    "# Laws CSV: ~45MB, ~269K rows\n",
    "# Build time: ~30 seconds | Load from cache: <1 second\n",
    "\n",
    "laws_index = get_or_build_index(\n",
    "    name=\"laws\",\n",
    "    csv_path=LAWS_CSV,\n",
    "    index_path=LAWS_INDEX_PATH,\n",
    "    force_rebuild=FORCE_REBUILD_INDICES,\n",
    "    # max_rows=10000  # Uncomment to test with smaller corpus\n",
    ")\n",
    "print(f\"\\nLaws index: {len(laws_index.documents):,} documents\")\n",
    "\n",
    "# Test search\n",
    "test_results = laws_index.search(\"Vertrag\", top_k=3)\n",
    "print(f\"\\nTest search 'Vertrag': {len(test_results)} results\")\n",
    "if test_results:\n",
    "    print(f\"  Top result: {test_results[0].get('citation', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or build courts index\n",
    "# Courts CSV: ~2.3GB, ~2.5M rows\n",
    "# Full corpus build time: ~15-20 minutes | Load from cache: ~10 seconds\n",
    "# Full corpus can have peak memory during build: ~8-16GB\n",
    "\n",
    "courts_index = get_or_build_index(\n",
    "    name=\"courts\",\n",
    "    csv_path=COURTS_CSV,\n",
    "    index_path=COURTS_INDEX_PATH,\n",
    "    force_rebuild=FORCE_REBUILD_INDICES,\n",
    "    max_rows=100000  # Change to use bigger corpus\n",
    ")\n",
    "print(f\"\\nCourts index: {len(courts_index.documents):,} documents\")\n",
    "\n",
    "# Test search\n",
    "test_results = courts_index.search(\"Meinungsfreiheit\", top_k=3)\n",
    "print(f\"\\nTest search 'Meinungsfreiheit': {len(test_results)} results\")\n",
    "if test_results:\n",
    "    print(f\"  Top result: {test_results[0].get('citation', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Search Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LawSearchTool:\n",
    "    \"\"\"Tool for searching Swiss federal laws corpus.\n",
    "\n",
    "    Searches the SR (Systematische Rechtssammlung) collection\n",
    "    using BM25 keyword matching.\n",
    "    \"\"\"\n",
    "\n",
    "    name: str = \"search_laws\"\n",
    "    description: str = \"\"\"Search Swiss federal laws (SR/Systematische Rechtssammlung) by keywords.\n",
    "Input: Search query string (can be in German, French, Italian, or English)\n",
    "Output: List of relevant law citations with text excerpts\n",
    "\n",
    "Use this tool to find relevant federal law provisions for a legal question.\n",
    "Example queries: \"contract formation requirements\", \"Vertragsabschluss\", \"divorce grounds\"\n",
    "\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        index: BM25Index,\n",
    "        top_k: int = 5,\n",
    "        max_excerpt_length: int = 300,\n",
    "    ):\n",
    "        \"\"\"Initialize law search tool.\n",
    "\n",
    "        Args:\n",
    "            index: BM25Index for federal laws corpus\n",
    "            top_k: Number of results to return\n",
    "            max_excerpt_length: Maximum characters for text excerpts\n",
    "        \"\"\"\n",
    "        self.index = index\n",
    "        self.top_k = top_k\n",
    "        self.max_excerpt_length = max_excerpt_length\n",
    "        self._last_results: list[dict] = []\n",
    "\n",
    "    def __call__(self, query: str) -> str:\n",
    "        \"\"\"Execute search and return formatted results.\n",
    "\n",
    "        Args:\n",
    "            query: Search query string\n",
    "\n",
    "        Returns:\n",
    "            Formatted string with search results\n",
    "        \"\"\"\n",
    "        return self.run(query)\n",
    "\n",
    "    def run(self, query: str) -> str:\n",
    "        \"\"\"Execute search and return formatted results.\n",
    "\n",
    "        Args:\n",
    "            query: Search query string\n",
    "\n",
    "        Returns:\n",
    "            Formatted string with search results\n",
    "        \"\"\"\n",
    "        if not query or not query.strip():\n",
    "            self._last_results = []\n",
    "            return \"Error: Empty query. Please provide search terms.\"\n",
    "\n",
    "        results = self.index.search(query, top_k=self.top_k)\n",
    "        self._last_results = results\n",
    "\n",
    "        if not results:\n",
    "            return f\"No relevant federal laws found for: '{query}'\"\n",
    "\n",
    "        formatted = []\n",
    "        for doc in results:\n",
    "            citation = doc.get(\"citation\", \"Unknown\")\n",
    "            text = doc.get(\"text\", \"\")\n",
    "\n",
    "            # Truncate text for readability\n",
    "            if len(text) > self.max_excerpt_length:\n",
    "                text = text[: self.max_excerpt_length] + \"...\"\n",
    "\n",
    "            formatted.append(f\"- {citation}: {text}\")\n",
    "\n",
    "        return \"\\n\".join(formatted)\n",
    "\n",
    "    def get_last_citations(self) -> list[str]:\n",
    "        \"\"\"Return citations from the last search.\n",
    "\n",
    "        Returns:\n",
    "            List of citation strings from the most recent search\n",
    "        \"\"\"\n",
    "        return [doc.get(\"citation\", \"\") for doc in self._last_results if doc.get(\"citation\")]\n",
    "\n",
    "\n",
    "class CourtSearchTool:\n",
    "    \"\"\"Tool for searching Swiss Federal Court decisions corpus.\n",
    "\n",
    "    Searches court decisions (BGE and docket-style citations)\n",
    "    using BM25 keyword matching.\n",
    "    \"\"\"\n",
    "\n",
    "    name: str = \"search_courts\"\n",
    "    description: str = \"\"\"Search Swiss Federal Court decisions by keywords.\n",
    "Input: Search query string (German, French, Italian, or English)\n",
    "Output: List of relevant court decision citations with excerpts\n",
    "\n",
    "Use this tool to find relevant case law and judicial interpretations.\n",
    "Example queries: \"negligence standard of care\", \"Sorgfaltspflicht\", \"contract interpretation\"\n",
    "\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        index: BM25Index,\n",
    "        top_k: int = 5,\n",
    "        max_excerpt_length: int = 300,\n",
    "    ):\n",
    "        \"\"\"Initialize court search tool.\n",
    "\n",
    "        Args:\n",
    "            index: BM25Index for court decisions corpus\n",
    "            top_k: Number of results to return\n",
    "            max_excerpt_length: Maximum characters for text excerpts\n",
    "        \"\"\"\n",
    "        self.index = index\n",
    "        self.top_k = top_k\n",
    "        self.max_excerpt_length = max_excerpt_length\n",
    "        self._last_results: list[dict] = []\n",
    "\n",
    "    def __call__(self, query: str) -> str:\n",
    "        \"\"\"Execute search and return formatted results.\n",
    "\n",
    "        Args:\n",
    "            query: Search query string\n",
    "\n",
    "        Returns:\n",
    "            Formatted string with search results\n",
    "        \"\"\"\n",
    "        return self.run(query)\n",
    "\n",
    "    def run(self, query: str) -> str:\n",
    "        \"\"\"Execute search and return formatted results.\n",
    "\n",
    "        Args:\n",
    "            query: Search query string\n",
    "\n",
    "        Returns:\n",
    "            Formatted string with search results\n",
    "        \"\"\"\n",
    "        if not query or not query.strip():\n",
    "            self._last_results = []\n",
    "            return \"Error: Empty query. Please provide search terms.\"\n",
    "\n",
    "        results = self.index.search(query, top_k=self.top_k)\n",
    "        self._last_results = results\n",
    "\n",
    "        if not results:\n",
    "            return f\"No relevant court decisions found for: '{query}'\"\n",
    "\n",
    "        formatted = []\n",
    "        for doc in results:\n",
    "            citation = doc.get(\"citation\", \"Unknown\")\n",
    "            text = doc.get(\"text\", \"\")\n",
    "\n",
    "            # Truncate text for readability\n",
    "            if len(text) > self.max_excerpt_length:\n",
    "                text = text[: self.max_excerpt_length] + \"...\"\n",
    "\n",
    "            formatted.append(f\"- {citation}: {text}\")\n",
    "\n",
    "        return \"\\n\".join(formatted)\n",
    "\n",
    "    def get_last_citations(self) -> list[str]:\n",
    "        \"\"\"Return citations from the last search.\n",
    "\n",
    "        Returns:\n",
    "            List of citation strings from the most recent search\n",
    "        \"\"\"\n",
    "        return [doc.get(\"citation\", \"\") for doc in self._last_results if doc.get(\"citation\")]\n",
    "\n",
    "\n",
    "# Create tools\n",
    "law_tool = LawSearchTool(\n",
    "    index=laws_index,\n",
    "    top_k=CONFIG[\"top_k_laws\"],\n",
    "    max_excerpt_length=300,\n",
    ")\n",
    "\n",
    "court_tool = CourtSearchTool(\n",
    "    index=courts_index,\n",
    "    top_k=CONFIG[\"top_k_courts\"],\n",
    "    max_excerpt_length=300,\n",
    ")\n",
    "\n",
    "# Tool registry\n",
    "TOOLS = {\n",
    "    \"search_laws\": law_tool,\n",
    "    \"search_courts\": court_tool,\n",
    "}\n",
    "\n",
    "print(\"Tools registered:\")\n",
    "for name, tool in TOOLS.items():\n",
    "    print(f\"  - {name}: {tool.description.split(chr(10))[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tools\n",
    "print(\"Testing law search:\")\n",
    "print(law_tool(\"Vertrag Abschluss\"))\n",
    "\n",
    "print(\"\\nTesting court search:\")\n",
    "print(court_tool(\"Meinungsfreiheit\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import importlib.util\n",
    "\n",
    "\n",
    "def has_cuda_support() -> bool:\n",
    "    \"\"\"Check if llama-cpp-python was built with CUDA support.\n",
    "\n",
    "    Returns:\n",
    "        True if CUDA support is available, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spec = importlib.util.find_spec(\"llama_cpp\")\n",
    "        if spec and spec.origin:\n",
    "            lib_dir = Path(spec.origin).parent\n",
    "            # Check for CUDA shared libraries in main dir and lib/ subdirectory\n",
    "            cuda_libs = (\n",
    "                list(lib_dir.glob(\"*cuda*\"))\n",
    "                + list(lib_dir.glob(\"*cublas*\"))\n",
    "                + list((lib_dir / \"lib\").glob(\"*cuda*\"))\n",
    "                + list((lib_dir / \"lib\").glob(\"*cublas*\"))\n",
    "            )\n",
    "            if cuda_libs:\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_device_info(n_gpu_layers: int) -> str:\n",
    "    \"\"\"Get human-readable device info string.\n",
    "\n",
    "    Args:\n",
    "        n_gpu_layers: Number of GPU layers configured\n",
    "\n",
    "    Returns:\n",
    "        String describing the compute device\n",
    "    \"\"\"\n",
    "    if n_gpu_layers == -1:\n",
    "        return \"GPU (all layers offloaded)\"\n",
    "    elif n_gpu_layers > 0:\n",
    "        return f\"GPU ({n_gpu_layers} layers offloaded)\"\n",
    "    else:\n",
    "        return \"CPU\"\n",
    "\n",
    "# Find model file\n",
    "model_file = MODEL_PATH / CONFIG[\"model_file\"]\n",
    "\n",
    "if not model_file.exists():\n",
    "    gguf_files = list(MODEL_PATH.glob(\"*.gguf\")) + list(MODEL_PATH.rglob(\"*.gguf\"))\n",
    "    if gguf_files:\n",
    "        model_file = gguf_files[0]\n",
    "        print(f\"Using model: {model_file}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No model found. Please download a GGUF model to {MODEL_PATH}\"\n",
    "        )\n",
    "\n",
    "print(f\"Loading model: {model_file}\")\n",
    "\n",
    "# Auto-detect GPU: use GPU if available, else CPU\n",
    "n_gpu_layers = CONFIG[\"n_gpu_layers\"]\n",
    "if n_gpu_layers == -1 and not has_cuda_support():\n",
    "    n_gpu_layers = 0  # Fallback to CPU if no CUDA support\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=str(model_file),\n",
    "    n_ctx=CONFIG[\"n_ctx\"],\n",
    "    n_threads=CONFIG[\"n_threads\"],\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Running on: {get_device_info(n_gpu_layers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "AGENT_SYSTEM_PROMPT = \"\"\"Du bist ein Schweizer Rechtsrecherche-Assistent mit Zugang zu zwei Such-Tools:\n",
    "\n",
    "1. search_laws(query): Durchsuche Schweizer Bundesgesetze (SR/Systematische Rechtssammlung)\n",
    "   - Gibt relevante Gesetzesbestimmungen mit Zitaten und Textauszügen zurück\n",
    "   - Verwende für Gesetzesrecht: Kodizes, Gesetze, Verordnungen\n",
    "\n",
    "2. search_courts(query): Durchsuche Schweizer Bundesgerichtsentscheide (BGE)\n",
    "   - Gibt relevante Rechtsprechung mit Zitaten und Auszügen zurück\n",
    "   - Verwende für Gerichtsentscheide und Präzedenzfälle\n",
    "\n",
    "WICHTIG: Suche IMMER auf Deutsch, da die Dokumente auf Deutsch sind.\n",
    "\n",
    "Deine Aufgabe: Rufe die Such-Tools auf, um relevante Schweizer Rechtszitate zu finden.\n",
    "\n",
    "Anleitung:\n",
    "- Durchsuche BEIDE: Gesetze UND Gerichtsentscheide\n",
    "- Verwende mehrere Suchanfragen mit deutschen Rechtsbegriffen\n",
    "- Rufe die Tools auf bis alle relevanten Quellen gefunden sind\n",
    "\n",
    "Antwortformat:\n",
    "Thought: [Deine Überlegung zur nächsten Suche]\n",
    "Action: [tool_name]\n",
    "Action Input: [deutsche Suchanfrage]\n",
    "\n",
    "=== BEISPIELE ===\n",
    "\n",
    "Beispiel 1 - Vertragsrecht:\n",
    "Query: What are the requirements for a valid contract?\n",
    "\n",
    "Thought: Ich suche nach Vertragsvoraussetzungen im Obligationenrecht.\n",
    "Action: search_laws\n",
    "Action Input: Vertrag Abschluss Voraussetzungen OR\n",
    "\n",
    "Observation: - Art. 1 Abs. 1 OR: Zum Abschluss eines Vertrages...\n",
    "\n",
    "Thought: Jetzt suche ich nach BGE-Entscheiden zum Vertragsschluss.\n",
    "Action: search_courts\n",
    "Action Input: Vertragsabschluss Gültigkeit Voraussetzungen\n",
    "\n",
    "Observation: - BGE 127 III 248 E. 3.1: Die Voraussetzungen...\n",
    "\n",
    "Thought: Ich suche nach weiteren Aspekten wie Willensmängel.\n",
    "Action: search_laws\n",
    "Action Input: Irrtum Täuschung Vertrag OR\n",
    "\n",
    "Beispiel 2 - Strafrecht:\n",
    "Query: Under what conditions can someone be held liable for negligent homicide?\n",
    "\n",
    "Thought: Ich suche nach fahrlässiger Tötung im Strafgesetzbuch.\n",
    "Action: search_laws\n",
    "Action Input: fahrlässige Tötung StGB\n",
    "\n",
    "Observation: - Art. 117 StGB: Wer fahrlässig den Tod...\n",
    "\n",
    "Thought: Ich suche nach Rechtsprechung zur Sorgfaltspflicht.\n",
    "Action: search_courts\n",
    "Action Input: fahrlässige Tötung Sorgfaltspflicht\n",
    "\n",
    "Observation: - BGE 135 IV 56 E. 2.1: Die Sorgfaltspflicht...\n",
    "\n",
    "Thought: Ich suche nach weiteren BGE zu Fahrlässigkeitsmassstäben.\n",
    "Action: search_courts\n",
    "Action Input: Fahrlässigkeit Verschulden Massstab\n",
    "\n",
    "Beispiel 3 - Familienrecht:\n",
    "Query: How is child custody determined after divorce?\n",
    "\n",
    "Thought: Ich suche nach Sorgerecht bei Scheidung im ZGB.\n",
    "Action: search_laws\n",
    "Action Input: Scheidung Sorgerecht Kinder ZGB\n",
    "\n",
    "Observation: - Art. 133 Abs. 1 ZGB: Das Gericht regelt...\n",
    "\n",
    "Thought: Ich suche nach BGE-Entscheiden zum Kindeswohl.\n",
    "Action: search_courts\n",
    "Action Input: Kindeswohl Obhut Zuteilung\n",
    "\n",
    "Observation: - BGE 142 III 481 E. 2.6: Das Kindeswohl...\n",
    "\n",
    "Thought: Ich suche nach weiteren Bestimmungen zur elterlichen Sorge.\n",
    "Action: search_laws\n",
    "Action Input: elterliche Sorge Zuteilung ZGB\n",
    "\n",
    "Beispiel 4 - Mietrecht:\n",
    "Query: When can a landlord terminate a lease?\n",
    "\n",
    "Thought: Ich suche nach Kündigungsrecht im Mietrecht.\n",
    "Action: search_laws\n",
    "Action Input: Mietvertrag Kündigung Vermieter OR\n",
    "\n",
    "Observation: - Art. 266a OR: Die Kündigung ist...\n",
    "\n",
    "Thought: Ich suche nach BGE zur missbräuchlichen Kündigung.\n",
    "Action: search_courts\n",
    "Action Input: Miete Kündigung missbräuchlich\n",
    "\n",
    "Observation: - BGE 140 III 496 E. 4.1: Eine Kündigung ist...\n",
    "\n",
    "Thought: Ich suche nach Kündigungsschutz.\n",
    "Action: search_laws\n",
    "Action Input: Kündigungsschutz Miete OR\n",
    "\n",
    "=== ENDE BEISPIELE ===\n",
    "\n",
    "Suche IMMER auf Deutsch. Rufe beide Tools (search_laws UND search_courts) auf.\"\"\"\n",
    "\n",
    "\n",
    "def parse_all_agent_actions(response: str) -> list[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Parse ALL action/input pairs from agent response.\n",
    "    \n",
    "    The LLM may output multiple actions in one response. This function\n",
    "    extracts all of them.\n",
    "    \n",
    "    Args:\n",
    "        response: Full LLM response text\n",
    "        \n",
    "    Returns:\n",
    "        List of (action, action_input) tuples\n",
    "    \"\"\"\n",
    "    actions = []\n",
    "    \n",
    "    # Find all \"Action:\" lines\n",
    "    action_pattern = r\"Action:\\s*(\\w+)\"\n",
    "    input_pattern = r\"Action Input:\\s*(.+?)(?=\\nAction:|$)\"\n",
    "    \n",
    "    # Find all action matches with their positions\n",
    "    action_matches = list(re.finditer(action_pattern, response, re.IGNORECASE))\n",
    "    \n",
    "    for i, action_match in enumerate(action_matches):\n",
    "        action = action_match.group(1).strip()\n",
    "        \n",
    "        # Find the corresponding Action Input\n",
    "        # Start search after the Action line\n",
    "        start_pos = action_match.end()\n",
    "        # End search at next Action or end of string\n",
    "        if i + 1 < len(action_matches):\n",
    "            end_pos = action_matches[i + 1].start()\n",
    "        else:\n",
    "            end_pos = len(response)\n",
    "        \n",
    "        input_text = response[start_pos:end_pos]\n",
    "        input_match = re.search(input_pattern, input_text, re.IGNORECASE | re.DOTALL)\n",
    "        \n",
    "        if input_match:\n",
    "            action_input = input_match.group(1).strip()\n",
    "            actions.append((action, action_input))\n",
    "    \n",
    "    return actions\n",
    "\n",
    "\n",
    "def extract_citations_from_text(text: str) -> list[str]:\n",
    "    \"\"\"Extract citations from any text (tool output or final answer).\"\"\"\n",
    "    citations = []\n",
    "    \n",
    "    # SR pattern: SR followed by number (optionally with article)\n",
    "    sr_matches = re.findall(\n",
    "        r\"SR\\s*\\d{3}(?:\\.\\d+)?(?:\\s+Art\\.?\\s*\\d+[a-z]?)?\",\n",
    "        text,\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    citations.extend(sr_matches)\n",
    "    \n",
    "    # BGE pattern: BGE volume section page\n",
    "    bge_matches = re.findall(\n",
    "        r\"BGE\\s+\\d{1,3}\\s+[IVX]+[a-z]?\\s+\\d+(?:\\s+E\\.\\s*\\d+[a-z]?)?\",\n",
    "        text,\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    citations.extend(bge_matches)\n",
    "    \n",
    "    # Art. pattern: Art. X LAW (e.g., Art. 1 ZGB, Art. 41 OR)\n",
    "    art_matches = re.findall(\n",
    "        r\"Art\\.?\\s+\\d+[a-z]?\\s+(?:Abs\\.?\\s*\\d+\\s+)?[A-Z]{2,}\",\n",
    "        text,\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    citations.extend(art_matches)\n",
    "    \n",
    "    return list(set(citations))\n",
    "\n",
    "\n",
    "def truncate_observation_for_llm(observation: str, max_chars: int = 1200) -> str:\n",
    "    \"\"\"Truncate observation text for LLM context, preserving data elsewhere.\n",
    "    \n",
    "    This truncates only the text sent to the LLM in the conversation.\n",
    "    Full observations remain in logs and are used for citation extraction.\n",
    "    \n",
    "    Args:\n",
    "        observation: Full observation text\n",
    "        max_chars: Maximum characters to keep\n",
    "        \n",
    "    Returns:\n",
    "        Truncated observation text\n",
    "    \"\"\"\n",
    "    if len(observation) <= max_chars:\n",
    "        return observation\n",
    "    \n",
    "    # Truncate and add indicator\n",
    "    return observation[:max_chars] + f\"\\n... (truncated, {len(observation) - max_chars} chars remaining)\"\n",
    "\n",
    "\n",
    "def truncate_conversation(conversation: str, max_chars: int) -> str:\n",
    "    \"\"\"Truncate conversation to fit within token budget, keeping system prompt and recent context.\n",
    "    \n",
    "    Args:\n",
    "        conversation: Full conversation text\n",
    "        max_chars: Maximum characters to keep\n",
    "        \n",
    "    Returns:\n",
    "        Truncated conversation text\n",
    "    \"\"\"\n",
    "    if len(conversation) <= max_chars:\n",
    "        return conversation\n",
    "    \n",
    "    # Find the system prompt end marker and keep it\n",
    "    inst_end = conversation.find(\"[/INST]\")\n",
    "    if inst_end == -1:\n",
    "        # Fallback: keep last max_chars\n",
    "        return \"...\" + conversation[-max_chars:]\n",
    "    \n",
    "    system_part = conversation[:inst_end + 7]  # Include [/INST]\n",
    "    remaining_budget = max_chars - len(system_part) - 100  # Buffer for truncation marker\n",
    "    \n",
    "    if remaining_budget <= 0:\n",
    "        # System prompt itself is too long, just truncate from end\n",
    "        return conversation[-max_chars:]\n",
    "    \n",
    "    # Keep the most recent conversation\n",
    "    rest = conversation[inst_end + 7:]\n",
    "    if len(rest) > remaining_budget:\n",
    "        rest = \"\\n...[earlier conversation truncated]...\\n\" + rest[-remaining_budget:]\n",
    "    \n",
    "    return system_part + rest\n",
    "\n",
    "\n",
    "def run_agent(query: str, verbose: bool = False) -> tuple[list[str], list[dict]]:\n",
    "    \"\"\"Run ReAct agent to retrieve citations.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (citations, logs) where logs contains detailed execution information\n",
    "    \"\"\"\n",
    "    # Format with Mistral Instruct tags\n",
    "    conversation = f\"[INST] {AGENT_SYSTEM_PROMPT}\\n\\nQuery: {query}\\n\\nThought: [/INST]\"\n",
    "    all_citations = []\n",
    "    logs: list[dict] = []\n",
    "    \n",
    "    for iteration in range(CONFIG[\"max_iterations\"]):\n",
    "        # Truncate conversation if too long to avoid context window overflow\n",
    "        max_conv_chars = CONFIG.get(\"max_conversation_chars\", 28000)\n",
    "        conversation = truncate_conversation(conversation, max_conv_chars)\n",
    "        \n",
    "        # Get LLM response with error handling for context overflow\n",
    "        try:\n",
    "            response = llm(\n",
    "                conversation,\n",
    "                max_tokens=CONFIG[\"max_tokens\"],\n",
    "                temperature=CONFIG[\"temperature\"],\n",
    "                stop=[\"Observation:\", \"[INST]\", \"</s>\"],\n",
    "            )[\"choices\"][0][\"text\"]\n",
    "        except ValueError as e:\n",
    "            error_str = str(e).lower()\n",
    "            if \"exceed context window\" in error_str or \"requested tokens\" in error_str:\n",
    "                # Aggressively truncate and retry once\n",
    "                conversation = truncate_conversation(conversation, max_chars=20000)\n",
    "                try:\n",
    "                    response = llm(\n",
    "                        conversation,\n",
    "                        max_tokens=CONFIG[\"max_tokens\"],\n",
    "                        temperature=CONFIG[\"temperature\"],\n",
    "                        stop=[\"Observation:\", \"[INST]\", \"</s>\"],\n",
    "                    )[\"choices\"][0][\"text\"]\n",
    "                except ValueError as retry_error:\n",
    "                    # Give up, return citations found so far\n",
    "                    logs.append({\n",
    "                        \"type\": \"error\",\n",
    "                        \"iteration\": iteration + 1,\n",
    "                        \"error\": f\"Context overflow after retry: {retry_error}\",\n",
    "                    })\n",
    "                    break\n",
    "            else:\n",
    "                raise\n",
    "        \n",
    "        # For subsequent turns, we need to handle the conversation format\n",
    "        if iteration == 0:\n",
    "            conversation = f\"[INST] {AGENT_SYSTEM_PROMPT}\\n\\nQuery: {query} [/INST]\\n\\nThought:{response}\"\n",
    "        else:\n",
    "            conversation += response\n",
    "        \n",
    "        # Log LLM output\n",
    "        logs.append({\n",
    "            \"type\": \"llm_response\",\n",
    "            \"iteration\": iteration + 1,\n",
    "            \"response\": response,\n",
    "            \"response_trunc\": response[:500] if len(response) > 500 else response,\n",
    "        })\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n[Iteration {iteration + 1}] LLM output (trunc):\")\n",
    "            print(response[:500])\n",
    "        \n",
    "        # Parse all actions from response\n",
    "        actions = parse_all_agent_actions(response)\n",
    "        \n",
    "        # Log parsed actions\n",
    "        if actions:\n",
    "            logs.append({\n",
    "                \"type\": \"parse\",\n",
    "                \"iteration\": iteration + 1,\n",
    "                \"actions_count\": len(actions),\n",
    "                \"actions\": actions,\n",
    "            })\n",
    "            if verbose:\n",
    "                print(f\"\\n[Iteration {iteration + 1}] Parsed {len(actions)} action(s):\")\n",
    "                for action, action_input in actions:\n",
    "                    print(f\"  Action: {action}, Input: {action_input[:100]}\")\n",
    "        \n",
    "        # Execute all actions\n",
    "        observations = []\n",
    "        for action, action_input in actions:\n",
    "            action_lower = action.lower()\n",
    "            \n",
    "            if action_lower in TOOLS:\n",
    "                tool = TOOLS[action_lower]\n",
    "                observation = tool(action_input)\n",
    "                \n",
    "                # Extract citations from full observation (before truncation)\n",
    "                obs_citations = tool.get_last_citations()\n",
    "                all_citations.extend(obs_citations)\n",
    "                \n",
    "                # Truncate observation only for LLM conversation (preserve full data in logs)\n",
    "                obs_truncated = truncate_observation_for_llm(observation, CONFIG[\"max_observation_chars\"])\n",
    "                observations.append(f\"Tool {action_lower}: {obs_truncated}\")\n",
    "                \n",
    "                # Log tool execution with full observation\n",
    "                logs.append({\n",
    "                    \"type\": \"tool_execution\",\n",
    "                    \"iteration\": iteration + 1,\n",
    "                    \"tool\": action,\n",
    "                    \"query\": action_input,\n",
    "                    \"citations_found\": obs_citations,\n",
    "                    \"citations_count\": len(obs_citations),\n",
    "                    \"observation\": observation,\n",
    "                    \"observation_trunc\": observation[:500] if len(observation) > 500 else observation,\n",
    "                })\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"\\n[Tool: {action}]\")\n",
    "                    print(f\"  Query: {action_input}\")\n",
    "                    print(f\"  Citations found: {len(obs_citations)}\")\n",
    "                    if obs_citations:\n",
    "                        print(f\"  Citations: {obs_citations[:5]}\")\n",
    "                    print(f\"  Observation (trunc): {observation[:300]}\")\n",
    "            else:\n",
    "                error_msg = f\"Unknown tool '{action}'. Available: search_laws, search_courts\"\n",
    "                observations.append(f\"Tool {action_lower}: {error_msg}\")\n",
    "                logs.append({\n",
    "                    \"type\": \"tool_error\",\n",
    "                    \"iteration\": iteration + 1,\n",
    "                    \"tool\": action,\n",
    "                    \"error\": error_msg,\n",
    "                })\n",
    "        \n",
    "        # Add all observations to conversation\n",
    "        if observations:\n",
    "            conversation += \"\\n\" + \"\\n\".join(observations) + \"\\n\\n[INST] Continue your analysis. [/INST]\\n\\nThought:\"\n",
    "        \n",
    "        # Check for final answer AFTER executing all actions\n",
    "        if \"Final Answer:\" in response:\n",
    "            final_text = response.split(\"Final Answer:\")[-1].strip()\n",
    "            citations = extract_citations_from_text(final_text)\n",
    "            all_citations.extend(citations)\n",
    "            \n",
    "            logs.append({\n",
    "                \"type\": \"parse\",\n",
    "                \"iteration\": iteration + 1,\n",
    "                \"status\": \"final_answer_seen\",\n",
    "            })\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\n[Iteration {iteration + 1}] Final Answer detected\")\n",
    "            break\n",
    "        \n",
    "        # If no actions found and no final answer, try to extract citations from response\n",
    "        if not actions and \"Final Answer:\" not in response:\n",
    "            citations = extract_citations_from_text(response)\n",
    "            all_citations.extend(citations)\n",
    "            logs.append({\n",
    "                \"type\": \"parse\",\n",
    "                \"iteration\": iteration + 1,\n",
    "                \"status\": \"no_actions_found\",\n",
    "                \"citations_extracted\": citations,\n",
    "            })\n",
    "            break\n",
    "    \n",
    "    # Deduplicate citations\n",
    "    unique_citations = list(set(all_citations))\n",
    "    \n",
    "    logs.append({\n",
    "        \"type\": \"summary\",\n",
    "        \"total_iterations\": len(logs),\n",
    "        \"total_citations\": len(unique_citations),\n",
    "        \"citations\": unique_citations,\n",
    "    })\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Found citations:\")\n",
    "        for c in unique_citations:\n",
    "            print(f\"  - {c}\")\n",
    "    \n",
    "    return unique_citations, logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test agent with a sample query\n",
    "test_query = \"What are the requirements for a valid contract under Swiss law?\"\n",
    "print(f\"Query: {test_query}\")\n",
    "print(\"\\nRunning agent...\\n\")\n",
    "\n",
    "citations, logs = run_agent(test_query, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Found citations:\")\n",
    "for c in citations:\n",
    "    print(f\"  - {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load queries from the configured query file\n",
    "if not QUERY_FILE.exists():\n",
    "    raise FileNotFoundError(f\"Query file not found: {QUERY_FILE}\")\n",
    "\n",
    "test_df = pd.read_csv(QUERY_FILE)\n",
    "\n",
    "print(f\"Loaded {len(test_df)} queries from {QUERY_FILE}\")\n",
    "print(f\"Columns: {list(test_df.columns)}\")\n",
    "\n",
    "if IS_VALIDATION_MODE and \"gold_citations\" in test_df.columns:\n",
    "    print(f\"Gold citations available for evaluation\")\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Generate predictions\n",
    "predictions = []\n",
    "all_logs = []  # Store logs for all queries\n",
    "\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Running agent\"):\n",
    "    query_id = row[\"query_id\"]\n",
    "    query_text = row[\"query\"]\n",
    "    \n",
    "    # Run agent\n",
    "    raw_citations, logs = run_agent(query_text, verbose=False)\n",
    "    \n",
    "    # Store logs with query_id\n",
    "    all_logs.append({\n",
    "        \"query_id\": query_id,\n",
    "        \"query\": query_text,\n",
    "        \"logs\": logs,\n",
    "    })\n",
    "    \n",
    "    predictions.append({\n",
    "        \"query_id\": query_id,\n",
    "        \"predicted_citations\": \";\".join(raw_citations),\n",
    "    })\n",
    "\n",
    "print(f\"\\nGenerated predictions for {len(predictions)} queries\")\n",
    "print(f\"Collected logs for {len(all_logs)} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview predictions\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submission\n",
    "submission_path = OUTPUT_PATH / \"submission.csv\"\n",
    "predictions_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Submission saved to: {submission_path}\")\n",
    "print(f\"Total predictions: {len(predictions_df)}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample submission:\")\n",
    "print(predictions_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "\n",
    "\n",
    "def citation_f1(\n",
    "    predicted: Sequence[str],\n",
    "    gold: Sequence[str],\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"Compute F1 score for citation overlap on a single query.\n",
    "\n",
    "    Args:\n",
    "        predicted: List of predicted canonical citation IDs\n",
    "        gold: List of ground truth canonical citation IDs\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with precision, recall, and F1\n",
    "    \"\"\"\n",
    "    pred_set = set(predicted)\n",
    "    gold_set = set(gold)\n",
    "\n",
    "    # Edge case: both empty\n",
    "    if len(pred_set) == 0 and len(gold_set) == 0:\n",
    "        return {\"precision\": 1.0, \"recall\": 1.0, \"f1\": 1.0}\n",
    "\n",
    "    # Edge case: prediction empty but gold not\n",
    "    if len(pred_set) == 0:\n",
    "        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "\n",
    "    # Edge case: gold empty but prediction not\n",
    "    if len(gold_set) == 0:\n",
    "        return {\"precision\": 0.0, \"recall\": 1.0, \"f1\": 0.0}\n",
    "\n",
    "    true_positives = len(pred_set & gold_set)\n",
    "    precision = true_positives / len(pred_set)\n",
    "    recall = true_positives / len(gold_set)\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "\n",
    "def macro_f1(\n",
    "    predictions: Sequence[Sequence[str]],\n",
    "    gold: Sequence[Sequence[str]],\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"Compute Macro F1: average F1 across all queries.\n",
    "\n",
    "    This is the PRIMARY competition metric.\n",
    "\n",
    "    Args:\n",
    "        predictions: List of predicted citation lists (one per query)\n",
    "        gold: List of gold citation lists (one per query)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with macro precision, recall, and F1\n",
    "    \"\"\"\n",
    "    if len(predictions) != len(gold):\n",
    "        raise ValueError(f\"Length mismatch: {len(predictions)} predictions vs {len(gold)} gold\")\n",
    "\n",
    "    if len(predictions) == 0:\n",
    "        return {\"macro_precision\": 0.0, \"macro_recall\": 0.0, \"macro_f1\": 0.0}\n",
    "\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for pred, g in zip(predictions, gold):\n",
    "        scores = citation_f1(pred, g)\n",
    "        precision_scores.append(scores[\"precision\"])\n",
    "        recall_scores.append(scores[\"recall\"])\n",
    "        f1_scores.append(scores[\"f1\"])\n",
    "\n",
    "    n = len(f1_scores)\n",
    "    return {\n",
    "        \"macro_precision\": sum(precision_scores) / n,\n",
    "        \"macro_recall\": sum(recall_scores) / n,\n",
    "        \"macro_f1\": sum(f1_scores) / n,\n",
    "    }\n",
    "\n",
    "\n",
    "def micro_f1(\n",
    "    predictions: Sequence[Sequence[str]],\n",
    "    gold: Sequence[Sequence[str]],\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"Compute Micro F1: aggregate TP/FP/FN across all queries.\n",
    "\n",
    "    Args:\n",
    "        predictions: List of predicted citation lists (one per query)\n",
    "        gold: List of gold citation lists (one per query)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with micro precision, recall, and F1\n",
    "    \"\"\"\n",
    "    if len(predictions) != len(gold):\n",
    "        raise ValueError(f\"Length mismatch: {len(predictions)} predictions vs {len(gold)} gold\")\n",
    "\n",
    "    total_tp = 0\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "\n",
    "    for pred, g in zip(predictions, gold):\n",
    "        pred_set = set(pred)\n",
    "        gold_set = set(g)\n",
    "\n",
    "        tp = len(pred_set & gold_set)\n",
    "        fp = len(pred_set - gold_set)\n",
    "        fn = len(gold_set - pred_set)\n",
    "\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "\n",
    "    if total_tp + total_fp == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = total_tp / (total_tp + total_fp)\n",
    "\n",
    "    if total_tp + total_fn == 0:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        recall = total_tp / (total_tp + total_fn)\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return {\n",
    "        \"micro_precision\": precision,\n",
    "        \"micro_recall\": recall,\n",
    "        \"micro_f1\": f1,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_submission(\n",
    "    submission_df: pd.DataFrame,\n",
    "    gold_df: pd.DataFrame,\n",
    "    metrics: list[str] | None = None,\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"Evaluate a submission DataFrame against gold DataFrame.\n",
    "\n",
    "    Args:\n",
    "        submission_df: DataFrame with query_id and predicted_citations\n",
    "        gold_df: DataFrame with query_id and gold_citations\n",
    "        metrics: List of metrics to compute (default: all)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with requested metric scores\n",
    "    \"\"\"\n",
    "    citation_separator = \";\"\n",
    "    \n",
    "    def parse_citations(citation_string: str) -> list[str]:\n",
    "        \"\"\"Parse citation string into list (citations are already normalized).\"\"\"\n",
    "        if not citation_string or citation_string.strip() == \"\":\n",
    "            return []\n",
    "        return [c.strip() for c in citation_string.split(citation_separator) if c.strip()]\n",
    "\n",
    "    # Merge DataFrames\n",
    "    merged = pd.merge(\n",
    "        submission_df,\n",
    "        gold_df,\n",
    "        on=\"query_id\",\n",
    "        how=\"inner\",\n",
    "    )\n",
    "\n",
    "    # Parse citations\n",
    "    predictions = [\n",
    "        parse_citations(row.get(\"predicted_citations\", \"\")) for _, row in merged.iterrows()\n",
    "    ]\n",
    "    gold = [parse_citations(row.get(\"gold_citations\", \"\")) for _, row in merged.iterrows()]\n",
    "\n",
    "    # Compute all scores\n",
    "    all_scores = {}\n",
    "\n",
    "    macro_scores = macro_f1(predictions, gold)\n",
    "    micro_scores = micro_f1(predictions, gold)\n",
    "\n",
    "    all_scores.update(macro_scores)\n",
    "    all_scores.update(micro_scores)\n",
    "\n",
    "    # Log per-sample TP/FP/FN for each query\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PER-SAMPLE EVALUATION RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    for idx, (_, row) in enumerate(merged.iterrows()):\n",
    "        query_id = row[\"query_id\"]\n",
    "        pred_set = set(predictions[idx])\n",
    "        gold_set = set(gold[idx])\n",
    "        \n",
    "        true_positives = list(pred_set & gold_set)\n",
    "        false_positives = list(pred_set - gold_set)\n",
    "        false_negatives = list(gold_set - pred_set)\n",
    "        \n",
    "        print(f\"\\nQuery ID: {query_id}\")\n",
    "        print(f\"  True Positives ({len(true_positives)}): {true_positives}\")\n",
    "        print(f\"  False Positives ({len(false_positives)}): {false_positives}\")\n",
    "        print(f\"  False Negatives ({len(false_negatives)}): {false_negatives}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "    # Filter to requested metrics\n",
    "    if metrics:\n",
    "        metric_mapping = {\n",
    "            \"f1\": \"macro_f1\",\n",
    "            \"precision\": \"macro_precision\",\n",
    "            \"recall\": \"macro_recall\",\n",
    "            \"macro_f1\": \"macro_f1\",\n",
    "            \"micro_f1\": \"micro_f1\",\n",
    "        }\n",
    "        filtered = {}\n",
    "        for m in metrics:\n",
    "            key = metric_mapping.get(m, m)\n",
    "            if key in all_scores:\n",
    "                filtered[m] = all_scores[key]\n",
    "        return filtered\n",
    "\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Local Evaluation (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate if in validation mode with gold labels\n",
    "if IS_VALIDATION_MODE and \"gold_citations\" in test_df.columns:\n",
    "    # Join predictions with gold citations from the same file\n",
    "    eval_df = predictions_df.merge(\n",
    "        test_df[[\"query_id\", \"gold_citations\"]],\n",
    "        on=\"query_id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    \n",
    "    if len(eval_df) > 0:\n",
    "        scores = evaluate_submission(\n",
    "            eval_df[[\"query_id\", \"predicted_citations\"]],\n",
    "            eval_df[[\"query_id\", \"gold_citations\"]],\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"EVALUATION RESULTS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Queries evaluated: {len(eval_df)}\")\n",
    "        print(f\"\\nMacro F1 (PRIMARY): {scores['macro_f1']:.4f}\")\n",
    "        print(f\"Macro Precision:    {scores['macro_precision']:.4f}\")\n",
    "        print(f\"Macro Recall:       {scores['macro_recall']:.4f}\")\n",
    "        print(f\"\\nMicro F1:           {scores['micro_f1']:.4f}\")\n",
    "        print(f\"Micro Precision:    {scores['micro_precision']:.4f}\")\n",
    "        print(f\"Micro Recall:       {scores['micro_recall']:.4f}\")\n",
    "    else:\n",
    "        print(\"No overlapping queries for evaluation.\")\n",
    "else:\n",
    "    print(\"Skipping evaluation (not in validation mode or no gold labels available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This agentic retrieval baseline demonstrates a more sophisticated approach:\n",
    "\n",
    "1. **Tool-augmented generation**: The LLM can search actual legal corpora rather than relying solely on parametric knowledge.\n",
    "\n",
    "2. **ReAct-style reasoning**: The agent reasons about what to search, executes searches, observes results, and iterates.\n",
    "\n",
    "3. **Grounded citations**: Citations are extracted from actual search results, reducing hallucination.\n",
    "\n",
    "4. **Comprehensive search**: The agent searches both laws and court decisions for complete results.\n",
    "\n",
    "## Potential Improvements\n",
    "\n",
    "- **Better search**: Use semantic search (embeddings) instead of BM25\n",
    "- **Query expansion**: Generate multiple search queries in different languages\n",
    "- **Relevance filtering**: Add a step to verify citations are actually relevant\n",
    "- **Citation validation**: Check that generated citations exist in the corpus\n",
    "- **Multi-hop reasoning**: Follow citation chains to find related sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set\n",
    "TEST_QUERY_FILE = DATA_PATH / \"test.csv\"\n",
    "\n",
    "if TEST_QUERY_FILE.exists():\n",
    "    print(f\"Loading test set from {TEST_QUERY_FILE}\")\n",
    "    test_set_df = pd.read_csv(TEST_QUERY_FILE)\n",
    "    print(f\"Loaded {len(test_set_df)} test queries\")\n",
    "    print(f\"Columns: {list(test_set_df.columns)}\")\n",
    "    \n",
    "    # Generate predictions for test set\n",
    "    test_predictions = []\n",
    "    test_all_logs = []  # Store logs for all test queries\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RUNNING AGENT ON TEST SET\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for _, row in tqdm(test_set_df.iterrows(), total=len(test_set_df), desc=\"Running agent on test set\"):\n",
    "        query_id = row[\"query_id\"]\n",
    "        query_text = row[\"query\"]\n",
    "        \n",
    "        # Run agent\n",
    "        raw_citations, logs = run_agent(query_text, verbose=False)\n",
    "        \n",
    "        # Store logs with query_id\n",
    "        test_all_logs.append({\n",
    "            \"query_id\": query_id,\n",
    "            \"query\": query_text,\n",
    "            \"logs\": logs,\n",
    "        })\n",
    "        \n",
    "        test_predictions.append({\n",
    "            \"query_id\": query_id,\n",
    "            \"predicted_citations\": \";\".join(raw_citations),\n",
    "        })\n",
    "    \n",
    "    print(f\"\\nGenerated predictions for {len(test_predictions)} test queries\")\n",
    "    print(f\"Collected logs for {len(test_all_logs)} test queries\")\n",
    "    \n",
    "    # Create DataFrame and save test submission\n",
    "    test_predictions_df = pd.DataFrame(test_predictions)\n",
    "    test_submission_path = OUTPUT_PATH / \"test_submission.csv\"\n",
    "    test_predictions_df.to_csv(test_submission_path, index=False)\n",
    "    \n",
    "    print(f\"\\nTest submission saved to: {test_submission_path}\")\n",
    "    print(f\"Total test predictions: {len(test_predictions_df)}\")\n",
    "    print(\"\\nSample test submission:\")\n",
    "    print(test_predictions_df.head())\n",
    "else:\n",
    "    print(f\"Test set file not found: {TEST_QUERY_FILE}\")\n",
    "    print(\"Skipping test set processing.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
